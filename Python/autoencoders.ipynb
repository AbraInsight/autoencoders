{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "---\n",
    "\n",
    "We will explore the use of autoencoders for automatic feature engineering. The idea is to automatically learn a set of features from raw data that can be useful in supervised learning tasks such as in computer vision and insurance.\n",
    "\n",
    "## Computer Vision\n",
    "---\n",
    "\n",
    "We will use the MNIST dataset for this purpose where the raw data is a 2 dimensional tensor of pixel intensities per image. The image is our unit of analysis: We will predict the probability of each class for each image. This is a multiclass classification task and we will use the one against all AUROC score and accuracy score to assess model performance on the test fold.\n",
    "\n",
    "## Insurance\n",
    "---\n",
    "\n",
    "We will use a dataset from the R package \"insuranceData\" where the raw data is a 2 dimensional tensor of historical policy level information per policy-period combination. The policy-period combination is our unit of analysis: We will predict the probability of loss for each policy-period combination. This is a binary class classification task and we will use the AUROC score and accuracy score to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/samson/anaconda3/envs/autoencoders/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9822157098076685420\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1810374656\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13296979930656387060\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:c2:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Author: Hamaad Musharaf Shah.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import keras\n",
    "from keras import backend as bkend\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Flatten, convolutional, pooling\n",
    "from keras import metrics\n",
    "\n",
    "from autoencoders_keras.get_session import get_session\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "KTF.set_session(get_session(gpu_fraction=0.75, allow_soft_placement=True, log_device_placement=False))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from autoencoders_keras.vanilla_autoencoder import VanillaAutoencoder\n",
    "from autoencoders_keras.convolutional_autoencoder import ConvolutionalAutoencoder\n",
    "from autoencoders_keras.convolutional2D_autoencoder import Convolutional2DAutoencoder\n",
    "from autoencoders_keras.seq2seq_autoencoder import Seq2SeqAutoencoder\n",
    "from autoencoders_keras.variational_autoencoder import VariationalAutoencoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "importlib.reload(bkend)\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "mnist = mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = mnist\n",
    "X_train = np.reshape(X_train, [X_train.shape[0], X_train.shape[1] * X_train.shape[1]])\n",
    "X_test = np.reshape(X_test, [X_test.shape[0], X_test.shape[1] * X_test.shape[1]])\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "---\n",
    "\n",
    "We will use the Python machine learning library scikit-learn for data transformation and the classification task. Note that we will code the autoencoders as scikit-learn transformers such that they can be readily used by scikit-learn pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler_classifier = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "logistic = linear_model.LogisticRegression(random_state=666)\n",
    "lb = LabelBinarizer()\n",
    "lb = lb.fit(y_train.reshape(y_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: No Autoencoders\n",
    "---\n",
    "\n",
    "We run the MNIST dataset without using an autoencoder. The 2 dimensional tensor of pixel intensities per image for MNIST images are of dimension $\\mathbb{R}^{28 \\times 28}$. We reshape them as a 1 dimensional tensor of dimension $\\mathbb{R}^{784}$ per image. Therefore we have 784, i.e., $28 \\times 28 = 784$, features for this supervised learning task per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_base = Pipeline(steps=[(\"scaler_classifier\", scaler_classifier),\n",
    "                            (\"classifier\", logistic)])\n",
    "pipe_base = pipe_base.fit(X_train, y_train)\n",
    "\n",
    "auroc_base = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)),\n",
    "                           pipe_base.predict_proba(X_test), \n",
    "                           average=\"weighted\")\n",
    "\n",
    "acc_base = pipe_base.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task without autoencoders: %.6f%%.\" % (auroc_base * 100))\n",
    "print(\"The accuracy score for the MNIST classification task without autoencoders: %.6f%%.\" % (acc_base * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Vanilla Autoencoders\n",
    "---\n",
    "\n",
    "An autoencoder is an unsupervised learning technique where the objective is to learn a set of features that can be used to reconstruct the input data.\n",
    "\n",
    "Our input data is $X \\in \\mathbb{R}^{N \\times 784}$. An encoder function $E$ maps this to a set of $K$ features such that $E: \\mathbb{R}^{N \\times 784} \\rightarrow \\mathbb{R}^{N \\times K}$. A decoder function $D$ uses the set of $K$ features to reconstruct the input data such that $D: \\mathbb{R}^{N \\times K} \\rightarrow \\mathbb{R}^{N \\times 784}$. \n",
    "    \n",
    "Lets denote the reconstructed data as $\\tilde{X} = D(E(X))$. The goal is to learn the encoding and decoding functions such that we minimize the difference between the input data and the reconstructed data. An example for an objective function for this task can be the Mean Squared Error (MSE) such that $\\frac{1}{N}||\\tilde{X} - X||^{2}_{2}$. \n",
    "    \n",
    "We learn the encoding and decoding functions by minimizing the MSE using the parameters that define the encoding and decoding functions: The gradient of the MSE with respect to the parameters are calculated using the chain rule, i.e., backpropagation, and used to update the parameters via an optimization algorithm such as Stochastic Gradient Descent (SGD). \n",
    "\n",
    "Lets assume we have a single layer autoencoder using the Exponential Linear Unit (ELU) activation function, batch normalization, dropout and the Adaptive Moment (Adam) optimization algorithm. $B$ is the batch size, $K$ is the number of features.\n",
    "\n",
    "* **Exponential Linear Unit:** The activation function is smooth everywhere and avoids the vanishing gradient problem as the output takes on negative values when the input is negative. $\\alpha$ is taken to be $1.0$.\n",
    "\n",
    "    \\begin{align}\n",
    "    H_{\\alpha}(z) &= \n",
    "    \\begin{cases}\n",
    "    \\alpha\\left(\\exp(z) - 1\\right) &\\text{ if } z < 0 \\\\\n",
    "    z &\\text{ if } z \\geq 0\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{dH_{\\alpha}(z)}{dz} &= \n",
    "    \\begin{cases}\n",
    "    \\alpha\\left(\\exp(z)\\right) &\\text{ if } z < 0 \\\\\n",
    "    1 &\\text{ if } z \\geq 0\n",
    "    \\end{cases} \n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "* **Batch Normalization:** The idea is to transform the inputs into a hidden layer's activation functions. We standardize or normalize first using the mean and variance parameters on a per feature basis and then learn a set of scaling and shifting parameters on a per feature basis that transforms the data. The following equations describe this layer succintly: The parameters we learn in this layer are $\\left(\\mu_{j}, \\sigma_{j}^2, \\beta_{j}, \\gamma_{j}\\right) \\hspace{0.1cm} \\forall j \\in \\{1, \\dots, K\\}$.\n",
    "\n",
    "    \\begin{align}\n",
    "    \\mu_{j} &= \\frac{1}{B} \\sum_{i=1}^{B} X_{i,j} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\sigma_{j}^2 &= \\frac{1}{B} \\sum_{i=1}^{B} \\left(X_{i,j} - \\mu_{j}\\right)^2 \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\hat{X}_{:,j} &= \\frac{X_{:,j} - \\mu_{j}}{\\sqrt{\\sigma_{j}^2 + \\epsilon}} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    Z_{:,j} &= \\gamma_{j}\\hat{X}_{:,j} + \\beta_{j} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "* **Dropout:** This regularization technique simply drops the outputs from input and hidden units with a certain probability say $50\\%$. \n",
    "\n",
    "\n",
    "* **Adam Optimization Algorithm:** This adaptive algorithm combines ideas from the Momentum and RMSProp optimization algorithms. The goal is to have some memory of past gradients which can guide future parameters updates. The following equations for the algorithm succintly describe this method assuming $\\theta$ is our set of parameters to be learnt and $\\eta$ is the learning rate.\n",
    "\n",
    "    \\begin{align}\n",
    "    m &\\leftarrow \\beta_{1}m + \\left[\\left(1 - \\beta_{1}\\right)\\left(\\nabla_{\\theta}\\text{MSE}\\right)\\right] \\\\\n",
    "    s &\\leftarrow \\beta_{2}s + \\left[\\left(1 - \\beta_{2}\\right)\\left(\\nabla_{\\theta}\\text{MSE} \\otimes \\nabla_{\\theta}\\text{MSE} \\right)\\right] \\\\\n",
    "    \\theta &\\leftarrow \\theta - \\eta m \\oslash \\sqrt{s + \\epsilon}\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               784784    \n",
      "=================================================================\n",
      "Total params: 6,604,420\n",
      "Trainable params: 6,589,852\n",
      "Non-trainable params: 14,568\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/50\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.0478 - val_loss: 0.0177\n",
      "Epoch 2/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0212 - val_loss: 0.0150\n",
      "Epoch 3/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0192 - val_loss: 0.0133\n",
      "Epoch 4/50\n",
      "42000/42000 [==============================] - 8s 202us/step - loss: 0.0181 - val_loss: 0.0124\n",
      "Epoch 5/50\n",
      "42000/42000 [==============================] - 9s 207us/step - loss: 0.0174 - val_loss: 0.0117\n",
      "Epoch 6/50\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 0.0168 - val_loss: 0.0113\n",
      "Epoch 7/50\n",
      "42000/42000 [==============================] - 8s 201us/step - loss: 0.0165 - val_loss: 0.0110\n",
      "Epoch 8/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0161 - val_loss: 0.0106\n",
      "Epoch 9/50\n",
      "42000/42000 [==============================] - 9s 202us/step - loss: 0.0158 - val_loss: 0.0105\n",
      "Epoch 10/50\n",
      "42000/42000 [==============================] - 8s 199us/step - loss: 0.0156 - val_loss: 0.0104\n",
      "Epoch 11/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0154 - val_loss: 0.0101\n",
      "Epoch 12/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0152 - val_loss: 0.0103\n",
      "Epoch 13/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0151 - val_loss: 0.0099\n",
      "Epoch 14/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0148 - val_loss: 0.0098\n",
      "Epoch 15/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0148 - val_loss: 0.0096\n",
      "Epoch 16/50\n",
      "42000/42000 [==============================] - 9s 204us/step - loss: 0.0146 - val_loss: 0.0096\n",
      "Epoch 17/50\n",
      "42000/42000 [==============================] - 8s 200us/step - loss: 0.0145 - val_loss: 0.0096\n",
      "Epoch 18/50\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 0.0144 - val_loss: 0.0095\n",
      "Epoch 19/50\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 0.0143 - val_loss: 0.0095\n",
      "Epoch 20/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 21/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 22/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 23/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0138 - val_loss: 0.0092\n",
      "Epoch 24/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0137 - val_loss: 0.0091\n",
      "Epoch 25/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0136 - val_loss: 0.0090\n",
      "Epoch 26/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0135 - val_loss: 0.0089\n",
      "Epoch 27/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0135 - val_loss: 0.0090\n",
      "Epoch 28/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0134 - val_loss: 0.0088\n",
      "Epoch 29/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0133 - val_loss: 0.0088\n",
      "Epoch 30/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0132 - val_loss: 0.0088\n",
      "Epoch 31/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0131 - val_loss: 0.0087\n",
      "Epoch 32/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0130 - val_loss: 0.0086\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0130 - val_loss: 0.0087\n",
      "Epoch 34/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0129 - val_loss: 0.0086\n",
      "Epoch 35/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0129 - val_loss: 0.0083\n",
      "Epoch 36/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0128 - val_loss: 0.0085\n",
      "Epoch 37/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0127 - val_loss: 0.0083\n",
      "Epoch 38/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0127 - val_loss: 0.0083\n",
      "Epoch 39/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0126 - val_loss: 0.0083\n",
      "Epoch 40/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0125 - val_loss: 0.0083\n",
      "Epoch 41/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0125 - val_loss: 0.0081\n",
      "Epoch 42/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0124 - val_loss: 0.0082\n",
      "Epoch 43/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0124 - val_loss: 0.0080\n",
      "Epoch 44/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0123 - val_loss: 0.0082\n",
      "Epoch 45/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0122 - val_loss: 0.0082\n",
      "Epoch 46/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0122 - val_loss: 0.0080\n",
      "Epoch 47/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0121 - val_loss: 0.0080\n",
      "Epoch 48/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0121 - val_loss: 0.0080\n",
      "Epoch 49/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0121 - val_loss: 0.0079\n",
      "Epoch 50/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0120 - val_loss: 0.0079\n",
      "The AUROC score for the MNIST classification task with an autoencoder: 99.761064%.\n",
      "The accuracy score for the MNIST classification task with an autoencoder: 96.420000%.\n"
     ]
    }
   ],
   "source": [
    "autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                 n_epoch=50,\n",
    "                                 batch_size=100,\n",
    "                                 encoder_layers=3,\n",
    "                                 decoder_layers=3,\n",
    "                                 n_hidden_units=1000,\n",
    "                                 encoding_dim=500,\n",
    "                                 denoising=None)\n",
    "\n",
    "print(autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_autoencoder = Pipeline(steps=[(\"autoencoder\", autoencoder),\n",
    "                                   (\"scaler_classifier\", scaler_classifier),\n",
    "                                   (\"classifier\", logistic)])\n",
    "\n",
    "pipe_autoencoder = pipe_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                  pipe_autoencoder.predict_proba(X_test), \n",
    "                                  average=\"weighted\")\n",
    "    \n",
    "acc_autoencoder = pipe_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with an autoencoder: %.6f%%.\" % (auroc_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with an autoencoder: %.6f%%.\" % (acc_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Denoising Autoencoders\n",
    "---\n",
    "\n",
    "The idea here is to add some noise to the data and try to learn a set of robust features that can reconstruct the non-noisy data from the noisy data. The MSE objective functions is as follows, $\\frac{1}{N}||D(E(X + \\epsilon)) - X||^{2}_{2}$, where $\\epsilon$ is some noise term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise = 0.10 * np.reshape(np.random.uniform(low=0.0, \n",
    "                                            high=1.0, \n",
    "                                            size=X_train.shape[0] * X_train.shape[1]), \n",
    "                          [X_train.shape[0], X_train.shape[1]])\n",
    "\n",
    "denoising_autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                           n_epoch=50,\n",
    "                                           batch_size=100,\n",
    "                                           encoder_layers=3,\n",
    "                                           decoder_layers=3,\n",
    "                                           n_hidden_units=1000,\n",
    "                                           encoding_dim=500,\n",
    "                                           denoising=noise)\n",
    "\n",
    "print(denoising_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_denoising_autoencoder = Pipeline(steps=[(\"autoencoder\", denoising_autoencoder),\n",
    "                                             (\"scaler_classifier\", scaler_classifier),\n",
    "                                             (\"classifier\", logistic)])\n",
    "\n",
    "pipe_denoising_autoencoder = pipe_denoising_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_denoising_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                            pipe_denoising_autoencoder.predict_proba(X_test), \n",
    "                                            average=\"weighted\")\n",
    "\n",
    "acc_denoising_autoencoder = pipe_denoising_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a denoising autoencoder: %.6f%%.\" % (auroc_denoising_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a denoising autoencoder: %.6f%%.\" % (acc_denoising_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: 1 Dimensional Convolutional Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convolutional_autoencoder = ConvolutionalAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))),\n",
    "                                                     n_epoch=50,\n",
    "                                                     batch_size=100,\n",
    "                                                     encoder_layers=3,\n",
    "                                                     decoder_layers=3,\n",
    "                                                     filters=100,\n",
    "                                                     kernel_size=5,\n",
    "                                                     strides=1,\n",
    "                                                     pool_size=4,\n",
    "                                                     encoding_dim=100,\n",
    "                                                     denoising=None)\n",
    "\n",
    "print(convolutional_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_convolutional_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional_autoencoder),\n",
    "                                                 (\"scaler_classifier\", scaler_classifier),\n",
    "                                                 (\"classifier\", logistic)])\n",
    "\n",
    "pipe_convolutional_autoencoder = pipe_convolutional_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), \n",
    "                                                                    y_train)\n",
    "\n",
    "auroc_convolutional_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                                pipe_convolutional_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))])),\n",
    "                                                average=\"weighted\")\n",
    "\n",
    "acc_convolutional_autoencoder = pipe_convolutional_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a 1 dimensional convolutional autoencoder: %.6f%%.\" % (auroc_convolutional_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a 1 dimensional convolutional autoencoder: %.6f%%.\" % (acc_convolutional_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Sequence to Sequence Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seq2seq_autoencoder = Seq2SeqAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))),\n",
    "                                         n_epoch=50,\n",
    "                                         batch_size=100,\n",
    "                                         encoder_layers=3,\n",
    "                                         decoder_layers=3,\n",
    "                                         n_hidden_units=100,\n",
    "                                         encoding_dim=100,\n",
    "                                         stateful=False,\n",
    "                                         denoising=None)\n",
    "\n",
    "print(seq2seq_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_seq2seq_autoencoder = Pipeline(steps=[(\"autoencoder\", seq2seq_autoencoder),\n",
    "                                           (\"scaler_classifier\", scaler_classifier),\n",
    "                                           (\"classifier\", logistic)])\n",
    "\n",
    "pipe_seq2seq_autoencoder = pipe_seq2seq_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]),\n",
    "                                                        y_train)\n",
    "\n",
    "auroc_seq2seq_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                          pipe_seq2seq_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))])),\n",
    "                                          average=\"weighted\")\n",
    "\n",
    "acc_seq2seq_autoencoder = pipe_seq2seq_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to sequence autoencoder: %.6f%%.\" % (auroc_seq2seq_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to sequence autoencoder: %.6f%%.\" % (acc_seq2seq_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Variational Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoding_dim = 500\n",
    "\n",
    "variational_autoencoder = VariationalAutoencoder(n_feat=X_train.shape[1],\n",
    "                                                 n_epoch=50,\n",
    "                                                 batch_size=100,\n",
    "                                                 encoder_layers=3,\n",
    "                                                 decoder_layers=3,\n",
    "                                                 n_hidden_units=500,\n",
    "                                                 encoding_dim=encoding_dim,\n",
    "                                                 denoising=None)\n",
    "\n",
    "print(variational_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_variational_autoencoder = Pipeline(steps=[(\"autoencoder\", variational_autoencoder),\n",
    "                                               (\"scaler_classifier\", scaler_classifier),\n",
    "                                               (\"classifier\", logistic)])\n",
    "\n",
    "pipe_variational_autoencoder = pipe_variational_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_variational_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                              pipe_variational_autoencoder.predict_proba(X_test), \n",
    "                                              average=\"weighted\")\n",
    "\n",
    "acc_variational_autoencoder = pipe_variational_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to variational autoencoder: %.6f%%.\" % (auroc_variational_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to variational autoencoder: %.6f%%.\" % (acc_variational_autoencoder * 100))\n",
    "\n",
    "if encoding_dim == 2:\n",
    "    test_encoded_df = pd.DataFrame(pipe_variational_autoencoder.named_steps[\"autoencoder\"].encoder.predict(X_test))\n",
    "    test_encoded_df[\"Target\"] = y_test\n",
    "    test_encoded_df.columns.values[0:2] = [\"Encoding_1\", \"Encoding_2\"]\n",
    "\n",
    "    scaler_plot = MinMaxScaler(feature_range=(0.25, 0.75))\n",
    "    scaler_plot = scaler_plot.fit(test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]])\n",
    "    test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]] = scaler_plot.transform(test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]])\n",
    "\n",
    "    cluster_plot = ggplot(test_encoded_df) + \\\n",
    "    geom_point(aes(x=\"Encoding_1\", \n",
    "                   y=\"Encoding_2\", \n",
    "                   fill=\"factor(Target)\"),\n",
    "               size=1,\n",
    "               color = \"black\") + \\\n",
    "    xlab(\"Encoding dimension 1\") + \\\n",
    "    ylab(\"Encoding dimension 2\") + \\\n",
    "    ggtitle(\"Variational autoencoder with 2-dimensional encoding\") + \\\n",
    "    theme_matplotlib()\n",
    "    print(cluster_plot)\n",
    "\n",
    "    n = 15\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "    for i, xi in enumerate(grid_x):\n",
    "        for j, yi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = pipe_variational_autoencoder.named_steps[\"autoencoder\"].generator.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.title(\"Variational autoencoder with 2-dimensional encoding\\nGenerating new images\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: 2 Dimensional Convolutional Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convolutional2D_autoencoder = Convolutional2DAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1),\n",
    "                                                         n_epoch=5,\n",
    "                                                         batch_size=100,\n",
    "                                                         encoder_layers=3,\n",
    "                                                         decoder_layers=3,\n",
    "                                                         filters=25,\n",
    "                                                         kernel_size=5,\n",
    "                                                         strides=1,\n",
    "                                                         pool_size=4,\n",
    "                                                         denoising=None)\n",
    "\n",
    "print(convolutional2D_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_convolutional2D_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional2D_autoencoder),\n",
    "                                                   (\"scaler_classifier\", scaler_classifier),\n",
    "                                                   (\"classifier\", logistic)])\n",
    "\n",
    "pipe_convolutional2D_autoencoder = pipe_convolutional2D_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1]),\n",
    "                                                                        y_train)\n",
    "\n",
    "auroc_convolutional2D_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                                  pipe_convolutional2D_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1])),\n",
    "                                                  average=\"weighted\")\n",
    "\n",
    "acc_convolutional2D_autoencoder = pipe_convolutional2D_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_test.shape[1], 0.5)), int(math.pow(X_test.shape[1], 0.5)), 1]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to 2 dimensional convolutional autoencoder: %.6f%%.\" % (auroc_convolutional2D_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to 2 dimensional convolutional autoencoder: %.6f%%.\" % (acc_convolutional2D_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "\n",
    "1. Goodfellow, I., Bengio, Y. and Courville A. (2016). Deep Learning (MIT Press).\n",
    "2. Geron, A. (2017). Hands-On Machine Learning with Scikit-Learn & Tensorflow (O'Reilly).\n",
    "3. http://scikit-learn.org/stable/#\n",
    "4. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "5. https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano\n",
    "6. https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "7. https://keras.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: 2 Dimensional Convolutional Autoencoders for CIFAR\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 32, 32, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 50)        2450      \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 50)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 8, 8, 50)          200       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 50)          40050     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 3)         2403      \n",
      "=================================================================\n",
      "Total params: 206,315\n",
      "Trainable params: 205,709\n",
      "Non-trainable params: 606\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_57/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_57/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_56/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_56/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_55/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_55/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_54/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_54/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_53/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_53/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_5/Adam/gradients/batch_normalization_52/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_52/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "35000/35000 [==============================] - 45s 1ms/step - loss: 0.0138 - val_loss: 0.0146\n",
      "Epoch 2/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0067 - val_loss: 0.0056\n",
      "Epoch 3/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0058 - val_loss: 0.0047\n",
      "Epoch 4/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0053 - val_loss: 0.0044\n",
      "Epoch 5/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0050 - val_loss: 0.0034\n",
      "Epoch 6/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0047 - val_loss: 0.0032\n",
      "Epoch 7/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0046 - val_loss: 0.0036\n",
      "Epoch 8/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0044 - val_loss: 0.0029\n",
      "Epoch 9/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0044 - val_loss: 0.0030\n",
      "Epoch 10/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0043 - val_loss: 0.0032\n",
      "Epoch 11/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0042 - val_loss: 0.0026\n",
      "Epoch 12/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0041 - val_loss: 0.0026\n",
      "Epoch 13/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 14/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 15/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0040 - val_loss: 0.0026\n",
      "Epoch 16/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0039 - val_loss: 0.0027\n",
      "Epoch 17/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0040 - val_loss: 0.0029\n",
      "Epoch 18/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0039 - val_loss: 0.0025\n",
      "Epoch 19/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0039 - val_loss: 0.0027\n",
      "Epoch 20/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0038 - val_loss: 0.0026\n",
      "Epoch 21/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0038 - val_loss: 0.0024\n",
      "Epoch 22/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0038 - val_loss: 0.0022\n",
      "Epoch 23/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0038 - val_loss: 0.0024\n",
      "Epoch 24/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 25/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0038 - val_loss: 0.0024\n",
      "Epoch 26/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0038 - val_loss: 0.0023\n",
      "Epoch 27/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0038 - val_loss: 0.0022\n",
      "Epoch 28/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0022\n",
      "Epoch 29/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0025\n",
      "Epoch 30/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0025\n",
      "Epoch 31/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0022\n",
      "Epoch 32/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0022\n",
      "Epoch 33/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0022\n",
      "Epoch 34/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0025\n",
      "Epoch 35/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 36/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 37/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0021\n",
      "Epoch 38/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 39/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 40/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 41/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 42/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0022\n",
      "Epoch 43/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 44/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 45/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 46/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0037 - val_loss: 0.0023\n",
      "Epoch 47/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 48/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 49/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 50/50\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0036 - val_loss: 0.0029\n"
     ]
    }
   ],
   "source": [
    "cifar = cifar10.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = cifar\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "convolutional2D_autoencoder = Convolutional2DAutoencoder(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]),\n",
    "                                                         n_epoch=50,\n",
    "                                                         batch_size=100,\n",
    "                                                         encoder_layers=3,\n",
    "                                                         decoder_layers=3,\n",
    "                                                         filters=50,\n",
    "                                                         kernel_size=4,\n",
    "                                                         strides=1,\n",
    "                                                         pool_size=4,\n",
    "                                                         denoising=None)\n",
    "\n",
    "print(convolutional2D_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_convolutional2D_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional2D_autoencoder)])\n",
    "\n",
    "pipe_convolutional2D_autoencoder = pipe_convolutional2D_autoencoder.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Transfer Learning for CIFAR\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1000)              3073000   \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 3,724,398\n",
      "Trainable params: 3,714,654\n",
      "Non-trainable params: 9,744\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_67/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_67/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_66/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_66/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_65/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_65/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_7/Adam/gradients/batch_normalization_64/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_64/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 10s 215us/step - loss: 1.9232 - categorical_accuracy: 0.3029 - val_loss: 1.6623 - val_categorical_accuracy: 0.3980\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 7s 151us/step - loss: 1.7258 - categorical_accuracy: 0.3778 - val_loss: 1.5691 - val_categorical_accuracy: 0.4266\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 7s 150us/step - loss: 1.6466 - categorical_accuracy: 0.4065 - val_loss: 1.4905 - val_categorical_accuracy: 0.4694\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 1.5814 - categorical_accuracy: 0.4319 - val_loss: 1.4349 - val_categorical_accuracy: 0.4850\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 7s 151us/step - loss: 1.5326 - categorical_accuracy: 0.4548 - val_loss: 1.4031 - val_categorical_accuracy: 0.4976\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 1.5016 - categorical_accuracy: 0.4662 - val_loss: 1.3893 - val_categorical_accuracy: 0.5048\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.4700 - categorical_accuracy: 0.4797 - val_loss: 1.3590 - val_categorical_accuracy: 0.5130\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.4430 - categorical_accuracy: 0.4902 - val_loss: 1.3514 - val_categorical_accuracy: 0.5230\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.4207 - categorical_accuracy: 0.4965 - val_loss: 1.3232 - val_categorical_accuracy: 0.5290\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.4001 - categorical_accuracy: 0.5037 - val_loss: 1.3009 - val_categorical_accuracy: 0.5384\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.3753 - categorical_accuracy: 0.5167 - val_loss: 1.2871 - val_categorical_accuracy: 0.5388\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.3506 - categorical_accuracy: 0.5208 - val_loss: 1.2874 - val_categorical_accuracy: 0.5394\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 1.3340 - categorical_accuracy: 0.5308 - val_loss: 1.2872 - val_categorical_accuracy: 0.5386\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.3176 - categorical_accuracy: 0.5392 - val_loss: 1.2744 - val_categorical_accuracy: 0.5446\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.2997 - categorical_accuracy: 0.5430 - val_loss: 1.2502 - val_categorical_accuracy: 0.5502\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.2867 - categorical_accuracy: 0.5481 - val_loss: 1.2480 - val_categorical_accuracy: 0.5552\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 1.2722 - categorical_accuracy: 0.5570 - val_loss: 1.2501 - val_categorical_accuracy: 0.5588\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.2544 - categorical_accuracy: 0.5618 - val_loss: 1.2335 - val_categorical_accuracy: 0.5658\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 1.2428 - categorical_accuracy: 0.5656 - val_loss: 1.2227 - val_categorical_accuracy: 0.5662\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.2296 - categorical_accuracy: 0.5694 - val_loss: 1.2191 - val_categorical_accuracy: 0.5720\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.2156 - categorical_accuracy: 0.5735 - val_loss: 1.2168 - val_categorical_accuracy: 0.5712\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.2054 - categorical_accuracy: 0.5809 - val_loss: 1.2235 - val_categorical_accuracy: 0.5698\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 1.1916 - categorical_accuracy: 0.5861 - val_loss: 1.2187 - val_categorical_accuracy: 0.5686\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 1.1801 - categorical_accuracy: 0.5890 - val_loss: 1.2075 - val_categorical_accuracy: 0.5726\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.1644 - categorical_accuracy: 0.5960 - val_loss: 1.2102 - val_categorical_accuracy: 0.5748\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.1594 - categorical_accuracy: 0.5992 - val_loss: 1.2007 - val_categorical_accuracy: 0.5776\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.1426 - categorical_accuracy: 0.6030 - val_loss: 1.1976 - val_categorical_accuracy: 0.5726\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 1.1336 - categorical_accuracy: 0.6076 - val_loss: 1.2019 - val_categorical_accuracy: 0.5776\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.1173 - categorical_accuracy: 0.6132 - val_loss: 1.2009 - val_categorical_accuracy: 0.5740\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.1130 - categorical_accuracy: 0.6138 - val_loss: 1.2056 - val_categorical_accuracy: 0.5792\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 1.1132 - categorical_accuracy: 0.6130 - val_loss: 1.1944 - val_categorical_accuracy: 0.5828\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0977 - categorical_accuracy: 0.6198 - val_loss: 1.1925 - val_categorical_accuracy: 0.5860\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 1.0885 - categorical_accuracy: 0.6243 - val_loss: 1.1898 - val_categorical_accuracy: 0.5884\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0770 - categorical_accuracy: 0.6274 - val_loss: 1.1901 - val_categorical_accuracy: 0.5730\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0657 - categorical_accuracy: 0.6350 - val_loss: 1.1993 - val_categorical_accuracy: 0.5798\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0640 - categorical_accuracy: 0.6310 - val_loss: 1.2059 - val_categorical_accuracy: 0.5778\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0544 - categorical_accuracy: 0.6343 - val_loss: 1.1883 - val_categorical_accuracy: 0.5836\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0402 - categorical_accuracy: 0.6388 - val_loss: 1.1866 - val_categorical_accuracy: 0.5756\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0384 - categorical_accuracy: 0.6420 - val_loss: 1.1932 - val_categorical_accuracy: 0.5828\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0228 - categorical_accuracy: 0.6465 - val_loss: 1.1788 - val_categorical_accuracy: 0.5898\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0198 - categorical_accuracy: 0.6495 - val_loss: 1.1730 - val_categorical_accuracy: 0.5922\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0168 - categorical_accuracy: 0.6491 - val_loss: 1.1708 - val_categorical_accuracy: 0.5870\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 1.0064 - categorical_accuracy: 0.6536 - val_loss: 1.1709 - val_categorical_accuracy: 0.5956\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.0036 - categorical_accuracy: 0.6538 - val_loss: 1.1833 - val_categorical_accuracy: 0.5858\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 0.9926 - categorical_accuracy: 0.6572 - val_loss: 1.1706 - val_categorical_accuracy: 0.5900\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.9810 - categorical_accuracy: 0.6613 - val_loss: 1.1886 - val_categorical_accuracy: 0.5842\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.9796 - categorical_accuracy: 0.6609 - val_loss: 1.1920 - val_categorical_accuracy: 0.5854\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.9726 - categorical_accuracy: 0.6650 - val_loss: 1.1802 - val_categorical_accuracy: 0.5974\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 0.9637 - categorical_accuracy: 0.6675 - val_loss: 1.1762 - val_categorical_accuracy: 0.5900\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.9584 - categorical_accuracy: 0.6698 - val_loss: 1.1801 - val_categorical_accuracy: 0.5864\n",
      "0.91638615\n",
      "10000/10000 [==============================] - 1s 134us/step\n",
      "[1.2038054151535034, 0.58089999999999997]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    autoencoder = pipe_convolutional2D_autoencoder.named_steps[\"autoencoder\"].autoencoder\n",
    "\n",
    "    encoder_at_layer = 10\n",
    "    for layer in autoencoder.layers[0:encoder_at_layer]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    classifier = autoencoder.layers[encoder_at_layer - 1].output\n",
    "    classifier = Flatten()(classifier)\n",
    "    \n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=1000, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=500, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=200, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=100, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=100, activation=\"elu\")(classifier)\n",
    "\n",
    "    classifier = Dense(units=num_classes, activation=\"softmax\")(classifier)\n",
    "\n",
    "    model_final = Model(autoencoder.input, classifier)\n",
    "    model_final.compile(loss=\"categorical_crossentropy\", \n",
    "                        optimizer=keras.optimizers.Adam(), \n",
    "                        metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    model_final.summary()\n",
    "\n",
    "    model_final.fit(X_train, keras.utils.to_categorical(y_train, num_classes),\n",
    "                    validation_split=0.1,\n",
    "                    epochs=50,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "print(roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                    model_final.predict(X_test), \n",
    "                    average=\"weighted\"))\n",
    "\n",
    "print(model_final.evaluate(X_test, keras.utils.to_categorical(y_test, num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 32, 32, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 50)        2450      \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 50)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 32, 32, 50)        40050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 3200)              12800     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 1000)              3201000   \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 4,056,622\n",
      "Trainable params: 3,963,360\n",
      "Non-trainable params: 93,262\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_91/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_91/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_90/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_90/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_89/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_89/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_88/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_88/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_87/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_87/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_86/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_86/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_10/Adam/gradients/batch_normalization_85/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_85/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 43s 951us/step - loss: 1.5236 - categorical_accuracy: 0.4471 - val_loss: 2.1126 - val_categorical_accuracy: 0.3942\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 38s 850us/step - loss: 1.1780 - categorical_accuracy: 0.5855 - val_loss: 1.5633 - val_categorical_accuracy: 0.5196\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 38s 851us/step - loss: 1.0472 - categorical_accuracy: 0.6369 - val_loss: 1.2697 - val_categorical_accuracy: 0.5956\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 38s 852us/step - loss: 0.9684 - categorical_accuracy: 0.6679 - val_loss: 1.2342 - val_categorical_accuracy: 0.6078\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 38s 853us/step - loss: 0.8967 - categorical_accuracy: 0.6964 - val_loss: 0.8746 - val_categorical_accuracy: 0.7076\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 39s 856us/step - loss: 0.8439 - categorical_accuracy: 0.7158 - val_loss: 0.9621 - val_categorical_accuracy: 0.6820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.7970 - categorical_accuracy: 0.7313 - val_loss: 0.8683 - val_categorical_accuracy: 0.7166\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.7425 - categorical_accuracy: 0.7510 - val_loss: 0.7487 - val_categorical_accuracy: 0.7442\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.7068 - categorical_accuracy: 0.7638 - val_loss: 0.8148 - val_categorical_accuracy: 0.7320\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.6707 - categorical_accuracy: 0.7758 - val_loss: 0.8119 - val_categorical_accuracy: 0.7330\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.6462 - categorical_accuracy: 0.7824 - val_loss: 0.7766 - val_categorical_accuracy: 0.7498\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.6115 - categorical_accuracy: 0.7972 - val_loss: 0.7957 - val_categorical_accuracy: 0.7402\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.5853 - categorical_accuracy: 0.8048 - val_loss: 0.7706 - val_categorical_accuracy: 0.7508\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.5575 - categorical_accuracy: 0.8144 - val_loss: 0.7653 - val_categorical_accuracy: 0.7550\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.5425 - categorical_accuracy: 0.8209 - val_loss: 0.8073 - val_categorical_accuracy: 0.7428\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.5163 - categorical_accuracy: 0.8277 - val_loss: 0.7698 - val_categorical_accuracy: 0.7588\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.4906 - categorical_accuracy: 0.8355 - val_loss: 0.7162 - val_categorical_accuracy: 0.7748\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.4745 - categorical_accuracy: 0.8434 - val_loss: 0.7563 - val_categorical_accuracy: 0.7626\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.4550 - categorical_accuracy: 0.8514 - val_loss: 0.7793 - val_categorical_accuracy: 0.7558\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.4384 - categorical_accuracy: 0.8546 - val_loss: 0.7504 - val_categorical_accuracy: 0.7652\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.4242 - categorical_accuracy: 0.8587 - val_loss: 0.7582 - val_categorical_accuracy: 0.7678\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.4144 - categorical_accuracy: 0.8619 - val_loss: 0.7708 - val_categorical_accuracy: 0.7648\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.4034 - categorical_accuracy: 0.8679 - val_loss: 0.7787 - val_categorical_accuracy: 0.7634\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.3923 - categorical_accuracy: 0.8702 - val_loss: 0.7438 - val_categorical_accuracy: 0.7726\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.3706 - categorical_accuracy: 0.8771 - val_loss: 0.7291 - val_categorical_accuracy: 0.7774\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.3647 - categorical_accuracy: 0.8798 - val_loss: 0.8000 - val_categorical_accuracy: 0.7638\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.3551 - categorical_accuracy: 0.8848 - val_loss: 0.7464 - val_categorical_accuracy: 0.7716\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.3450 - categorical_accuracy: 0.8864 - val_loss: 0.7307 - val_categorical_accuracy: 0.7796\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.3350 - categorical_accuracy: 0.8909 - val_loss: 0.7554 - val_categorical_accuracy: 0.7742\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.3316 - categorical_accuracy: 0.8908 - val_loss: 0.7596 - val_categorical_accuracy: 0.7768\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.3192 - categorical_accuracy: 0.8962 - val_loss: 0.8105 - val_categorical_accuracy: 0.7688\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.3078 - categorical_accuracy: 0.8993 - val_loss: 0.7542 - val_categorical_accuracy: 0.7850\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.3045 - categorical_accuracy: 0.8996 - val_loss: 0.7451 - val_categorical_accuracy: 0.7842\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.2933 - categorical_accuracy: 0.9037 - val_loss: 0.7556 - val_categorical_accuracy: 0.7836\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.2911 - categorical_accuracy: 0.9051 - val_loss: 0.7712 - val_categorical_accuracy: 0.7794\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2876 - categorical_accuracy: 0.9063 - val_loss: 0.7790 - val_categorical_accuracy: 0.7778\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2776 - categorical_accuracy: 0.9094 - val_loss: 0.7708 - val_categorical_accuracy: 0.7844\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2745 - categorical_accuracy: 0.9116 - val_loss: 0.7661 - val_categorical_accuracy: 0.7794\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.2620 - categorical_accuracy: 0.9136 - val_loss: 0.8330 - val_categorical_accuracy: 0.7758\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.2595 - categorical_accuracy: 0.9160 - val_loss: 0.8483 - val_categorical_accuracy: 0.7618\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2607 - categorical_accuracy: 0.9138 - val_loss: 0.8518 - val_categorical_accuracy: 0.7690\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2542 - categorical_accuracy: 0.9178 - val_loss: 0.7974 - val_categorical_accuracy: 0.7798\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2436 - categorical_accuracy: 0.9193 - val_loss: 0.8758 - val_categorical_accuracy: 0.7674\n",
      "Epoch 44/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2455 - categorical_accuracy: 0.9195 - val_loss: 0.8451 - val_categorical_accuracy: 0.7764\n",
      "Epoch 45/50\n",
      "45000/45000 [==============================] - 38s 855us/step - loss: 0.2392 - categorical_accuracy: 0.9227 - val_loss: 0.8543 - val_categorical_accuracy: 0.7756\n",
      "Epoch 46/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2377 - categorical_accuracy: 0.9231 - val_loss: 0.8100 - val_categorical_accuracy: 0.7754\n",
      "Epoch 47/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2379 - categorical_accuracy: 0.9226 - val_loss: 0.8522 - val_categorical_accuracy: 0.7756\n",
      "Epoch 48/50\n",
      "45000/45000 [==============================] - 38s 854us/step - loss: 0.2302 - categorical_accuracy: 0.9266 - val_loss: 0.7964 - val_categorical_accuracy: 0.7822\n",
      "Epoch 49/50\n",
      "45000/45000 [==============================] - 38s 853us/step - loss: 0.2196 - categorical_accuracy: 0.9305 - val_loss: 0.8279 - val_categorical_accuracy: 0.7854\n",
      "Epoch 50/50\n",
      "45000/45000 [==============================] - 38s 851us/step - loss: 0.2276 - categorical_accuracy: 0.9254 - val_loss: 0.7885 - val_categorical_accuracy: 0.7868\n",
      "0.973161255556\n",
      "10000/10000 [==============================] - 5s 532us/step\n",
      "[0.81246538534164425, 0.77500000000000002]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    autoencoder = pipe_convolutional2D_autoencoder.named_steps[\"autoencoder\"].autoencoder\n",
    "\n",
    "    encoder_at_layer = 10\n",
    "    for layer in autoencoder.layers[0:encoder_at_layer]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    classifier = autoencoder.layers[encoder_at_layer - 1].output\n",
    "    \n",
    "    classifier = convolutional.UpSampling2D(4)(classifier)\n",
    "    \n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = convolutional.Conv2D(filters=50, kernel_size=4, strides=1, activation=\"elu\", padding=\"same\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = convolutional.Conv2D(filters=50, kernel_size=4, strides=1, activation=\"elu\", padding=\"same\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = convolutional.Conv2D(filters=50, kernel_size=4, strides=1, activation=\"elu\", padding=\"same\")(classifier)\n",
    "    classifier = pooling.MaxPooling2D(4, padding=\"same\")(classifier)\n",
    "    \n",
    "    classifier = Flatten()(classifier)\n",
    "    \n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=1000, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=500, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=200, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=100, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=100, activation=\"elu\")(classifier)\n",
    "\n",
    "    classifier = Dense(units=num_classes, activation=\"softmax\")(classifier)\n",
    "\n",
    "    model_final = Model(autoencoder.input, classifier)\n",
    "    model_final.compile(loss=\"categorical_crossentropy\", \n",
    "                        optimizer=keras.optimizers.Adam(), \n",
    "                        metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    model_final.summary()\n",
    "\n",
    "    model_final.fit(X_train, keras.utils.to_categorical(y_train, num_classes),\n",
    "                    validation_split=0.1,\n",
    "                    epochs=50,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "print(roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                    model_final.predict(X_test), \n",
    "                    average=\"weighted\"))\n",
    "\n",
    "print(model_final.evaluate(X_test, keras.utils.to_categorical(y_test, num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    autoencoder = pipe_convolutional2D_autoencoder.named_steps[\"autoencoder\"].autoencoder\n",
    "\n",
    "    classifier = autoencoder.layers[0].output\n",
    "    classifier = Flatten()(classifier)\n",
    "    \n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=1000, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=500, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=200, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=100, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=100, activation=\"elu\")(classifier)\n",
    "\n",
    "    classifier = Dense(units=num_classes, activation=\"softmax\")(classifier)\n",
    "\n",
    "    model_final = Model(autoencoder.input, classifier)\n",
    "    model_final.compile(loss=\"categorical_crossentropy\", \n",
    "                        optimizer=keras.optimizers.Adam(), \n",
    "                        metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    model_final.summary()\n",
    "\n",
    "    model_final.fit(X_train, keras.utils.to_categorical(y_train, num_classes),\n",
    "                    validation_split=0.1,\n",
    "                    epochs=50,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "print(roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                    model_final.predict(X_test), \n",
    "                    average=\"weighted\"))\n",
    "\n",
    "print(model_final.evaluate(X_test, keras.utils.to_categorical(y_test, num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    autoencoder = pipe_convolutional2D_autoencoder.named_steps[\"autoencoder\"].autoencoder\n",
    "\n",
    "    classifier = autoencoder.layers[0].output\n",
    "    \n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = convolutional.Conv2D(filters=50, kernel_size=4, strides=1, activation=\"elu\", padding=\"same\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = convolutional.Conv2D(filters=50, kernel_size=4, strides=1, activation=\"elu\", padding=\"same\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = convolutional.Conv2D(filters=50, kernel_size=4, strides=1, activation=\"elu\", padding=\"same\")(classifier)\n",
    "    classifier = pooling.MaxPooling2D(4, padding=\"same\")(classifier)\n",
    "    \n",
    "    classifier = Flatten()(classifier)\n",
    "    \n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=1000, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=500, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=200, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=100, activation=\"elu\")(classifier)\n",
    "    classifier = Dropout(rate=0.5)(classifier)\n",
    "\n",
    "    classifier = BatchNormalization()(classifier)\n",
    "    classifier = Dense(units=100, activation=\"elu\")(classifier)\n",
    "\n",
    "    classifier = Dense(units=num_classes, activation=\"softmax\")(classifier)\n",
    "\n",
    "    model_final = Model(autoencoder.input, classifier)\n",
    "    model_final.compile(loss=\"categorical_crossentropy\", \n",
    "                        optimizer=keras.optimizers.Adam(), \n",
    "                        metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    model_final.summary()\n",
    "\n",
    "    model_final.fit(X_train, keras.utils.to_categorical(y_train, num_classes),\n",
    "                    validation_split=0.1,\n",
    "                    epochs=50,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "print(roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                    model_final.predict(X_test), \n",
    "                    average=\"weighted\"))\n",
    "\n",
    "print(model_final.evaluate(X_test, keras.utils.to_categorical(y_test, num_classes)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
