{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "---\n",
    "\n",
    "We will explore the use of autoencoders for automatic feature engineering. The idea is to automatically learn a set of features from raw data that can be useful in supervised learning tasks such as in computer vision and insurance.\n",
    "\n",
    "## Computer Vision\n",
    "---\n",
    "\n",
    "We will use the MNIST dataset for this purpose where the raw data is a 2 dimensional tensor of pixel intensities per image. The image is our unit of analysis: We will predict the probability of each class for each image. This is a multiclass classification task and we will use the one against all AUROC score and accuracy score to assess model performance on the test fold.\n",
    "\n",
    "## Insurance\n",
    "---\n",
    "\n",
    "We will use a dataset from the R package \"insuranceData\" where the raw data is a 2 dimensional tensor of historical policy level information per policy-period combination. The policy-period combination is our unit of analysis: We will predict the probability of loss for each policy-period combination. This is a binary class classification task and we will use the AUROC score and accuracy score to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/samson/anaconda3/envs/autoencoders/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9693286546838735688\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1810374656\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 1298818725826307886\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:c2:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Author: Hamaad Musharaf Shah.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer, RobustScaler, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import keras\n",
    "from keras import backend as bkend\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Flatten, convolutional, pooling\n",
    "from keras import metrics\n",
    "\n",
    "from autoencoders_keras.get_session import get_session\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "KTF.set_session(get_session(gpu_fraction=0.75, allow_soft_placement=True, log_device_placement=False))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from autoencoders_keras.vanilla_autoencoder import VanillaAutoencoder\n",
    "from autoencoders_keras.convolutional_autoencoder import ConvolutionalAutoencoder\n",
    "from autoencoders_keras.convolutional2D_autoencoder import Convolutional2DAutoencoder\n",
    "from autoencoders_keras.seq2seq_autoencoder import Seq2SeqAutoencoder\n",
    "from autoencoders_keras.variational_autoencoder import VariationalAutoencoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "importlib.reload(bkend)\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "mnist = mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = mnist\n",
    "X_train = np.reshape(X_train, [X_train.shape[0], X_train.shape[1] * X_train.shape[1]])\n",
    "X_test = np.reshape(X_test, [X_test.shape[0], X_test.shape[1] * X_test.shape[1]])\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "---\n",
    "\n",
    "We will use the Python machine learning library scikit-learn for data transformation and the classification task. Note that we will code the autoencoders as scikit-learn transformers such that they can be readily used by scikit-learn pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler_classifier = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "logistic = linear_model.LogisticRegression(random_state=666)\n",
    "lb = LabelBinarizer()\n",
    "lb = lb.fit(y_train.reshape(y_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: No Autoencoders\n",
    "---\n",
    "\n",
    "We run the MNIST dataset without using an autoencoder. The 2 dimensional tensor of pixel intensities per image for MNIST images are of dimension $\\mathbb{R}^{28 \\times 28}$. We reshape them as a 1 dimensional tensor of dimension $\\mathbb{R}^{784}$ per image. Therefore we have 784, i.e., $28 \\times 28 = 784$, features for this supervised learning task per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_base = Pipeline(steps=[(\"scaler_classifier\", scaler_classifier),\n",
    "                            (\"classifier\", logistic)])\n",
    "pipe_base = pipe_base.fit(X_train, y_train)\n",
    "\n",
    "auroc_base = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)),\n",
    "                           pipe_base.predict_proba(X_test), \n",
    "                           average=\"weighted\")\n",
    "\n",
    "acc_base = pipe_base.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task without autoencoders: %.6f%%.\" % (auroc_base * 100))\n",
    "print(\"The accuracy score for the MNIST classification task without autoencoders: %.6f%%.\" % (acc_base * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Vanilla Autoencoders\n",
    "---\n",
    "\n",
    "An autoencoder is an unsupervised learning technique where the objective is to learn a set of features that can be used to reconstruct the input data.\n",
    "\n",
    "Our input data is $X \\in \\mathbb{R}^{N \\times 784}$. An encoder function $E$ maps this to a set of $K$ features such that $E: \\mathbb{R}^{N \\times 784} \\rightarrow \\mathbb{R}^{N \\times K}$. A decoder function $D$ uses the set of $K$ features to reconstruct the input data such that $D: \\mathbb{R}^{N \\times K} \\rightarrow \\mathbb{R}^{N \\times 784}$. \n",
    "    \n",
    "Lets denote the reconstructed data as $\\tilde{X} = D(E(X))$. The goal is to learn the encoding and decoding functions such that we minimize the difference between the input data and the reconstructed data. An example for an objective function for this task can be the Mean Squared Error (MSE) such that $\\frac{1}{N}||\\tilde{X} - X||^{2}_{2}$. \n",
    "    \n",
    "We learn the encoding and decoding functions by minimizing the MSE using the parameters that define the encoding and decoding functions: The gradient of the MSE with respect to the parameters are calculated using the chain rule, i.e., backpropagation, and used to update the parameters via an optimization algorithm such as Stochastic Gradient Descent (SGD). \n",
    "\n",
    "Lets assume we have a single layer autoencoder using the Exponential Linear Unit (ELU) activation function, batch normalization, dropout and the Adaptive Moment (Adam) optimization algorithm. $B$ is the batch size, $K$ is the number of features.\n",
    "\n",
    "* **Exponential Linear Unit:** The activation function is smooth everywhere and avoids the vanishing gradient problem as the output takes on negative values when the input is negative. $\\alpha$ is taken to be $1.0$.\n",
    "\n",
    "    \\begin{align}\n",
    "    H_{\\alpha}(z) &= \n",
    "    \\begin{cases}\n",
    "    \\alpha\\left(\\exp(z) - 1\\right) &\\text{ if } z < 0 \\\\\n",
    "    z &\\text{ if } z \\geq 0\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{dH_{\\alpha}(z)}{dz} &= \n",
    "    \\begin{cases}\n",
    "    \\alpha\\left(\\exp(z)\\right) &\\text{ if } z < 0 \\\\\n",
    "    1 &\\text{ if } z \\geq 0\n",
    "    \\end{cases} \n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "* **Batch Normalization:** The idea is to transform the inputs into a hidden layer's activation functions. We standardize or normalize first using the mean and variance parameters on a per feature basis and then learn a set of scaling and shifting parameters on a per feature basis that transforms the data. The following equations describe this layer succintly: The parameters we learn in this layer are $\\left(\\mu_{j}, \\sigma_{j}^2, \\beta_{j}, \\gamma_{j}\\right) \\hspace{0.1cm} \\forall j \\in \\{1, \\dots, K\\}$.\n",
    "\n",
    "    \\begin{align}\n",
    "    \\mu_{j} &= \\frac{1}{B} \\sum_{i=1}^{B} X_{i,j} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\sigma_{j}^2 &= \\frac{1}{B} \\sum_{i=1}^{B} \\left(X_{i,j} - \\mu_{j}\\right)^2 \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\hat{X}_{:,j} &= \\frac{X_{:,j} - \\mu_{j}}{\\sqrt{\\sigma_{j}^2 + \\epsilon}} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    Z_{:,j} &= \\gamma_{j}\\hat{X}_{:,j} + \\beta_{j} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "* **Dropout:** This regularization technique simply drops the outputs from input and hidden units with a certain probability say $50\\%$. \n",
    "\n",
    "\n",
    "* **Adam Optimization Algorithm:** This adaptive algorithm combines ideas from the Momentum and RMSProp optimization algorithms. The goal is to have some memory of past gradients which can guide future parameters updates. The following equations for the algorithm succintly describe this method assuming $\\theta$ is our set of parameters to be learnt and $\\eta$ is the learning rate.\n",
    "\n",
    "    \\begin{align}\n",
    "    m &\\leftarrow \\beta_{1}m + \\left[\\left(1 - \\beta_{1}\\right)\\left(\\nabla_{\\theta}\\text{MSE}\\right)\\right] \\\\\n",
    "    s &\\leftarrow \\beta_{2}s + \\left[\\left(1 - \\beta_{2}\\right)\\left(\\nabla_{\\theta}\\text{MSE} \\otimes \\nabla_{\\theta}\\text{MSE} \\right)\\right] \\\\\n",
    "    \\theta &\\leftarrow \\theta - \\eta m \\oslash \\sqrt{s + \\epsilon}\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               784784    \n",
      "=================================================================\n",
      "Total params: 6,604,420\n",
      "Trainable params: 6,589,852\n",
      "Non-trainable params: 14,568\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/50\n",
      "33100/42000 [======================>.......] - ETA: 2s - loss: 0.0534"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d2f18babfba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                    (\"classifier\", logistic)])\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpipe_autoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m auroc_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/autoencoders/Python/autoencoders_keras/vanilla_autoencoder.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     91\u001b[0m                                  \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                                  \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                                  verbose=1)\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/autoencoders/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                 n_epoch=50,\n",
    "                                 batch_size=100,\n",
    "                                 encoder_layers=3,\n",
    "                                 decoder_layers=3,\n",
    "                                 n_hidden_units=1000,\n",
    "                                 encoding_dim=500,\n",
    "                                 denoising=None)\n",
    "\n",
    "print(autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_autoencoder = Pipeline(steps=[(\"autoencoder\", autoencoder),\n",
    "                                   (\"scaler_classifier\", scaler_classifier),\n",
    "                                   (\"classifier\", logistic)])\n",
    "\n",
    "pipe_autoencoder = pipe_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                  pipe_autoencoder.predict_proba(X_test), \n",
    "                                  average=\"weighted\")\n",
    "    \n",
    "acc_autoencoder = pipe_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with an autoencoder: %.6f%%.\" % (auroc_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with an autoencoder: %.6f%%.\" % (acc_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Denoising Autoencoders\n",
    "---\n",
    "\n",
    "The idea here is to add some noise to the data and try to learn a set of robust features that can reconstruct the non-noisy data from the noisy data. The MSE objective functions is as follows, $\\frac{1}{N}||D(E(X + \\epsilon)) - X||^{2}_{2}$, where $\\epsilon$ is some noise term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise = 0.10 * np.reshape(np.random.uniform(low=0.0, \n",
    "                                            high=1.0, \n",
    "                                            size=X_train.shape[0] * X_train.shape[1]), \n",
    "                          [X_train.shape[0], X_train.shape[1]])\n",
    "\n",
    "denoising_autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                           n_epoch=50,\n",
    "                                           batch_size=100,\n",
    "                                           encoder_layers=3,\n",
    "                                           decoder_layers=3,\n",
    "                                           n_hidden_units=1000,\n",
    "                                           encoding_dim=500,\n",
    "                                           denoising=noise)\n",
    "\n",
    "print(denoising_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_denoising_autoencoder = Pipeline(steps=[(\"autoencoder\", denoising_autoencoder),\n",
    "                                             (\"scaler_classifier\", scaler_classifier),\n",
    "                                             (\"classifier\", logistic)])\n",
    "\n",
    "pipe_denoising_autoencoder = pipe_denoising_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_denoising_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                            pipe_denoising_autoencoder.predict_proba(X_test), \n",
    "                                            average=\"weighted\")\n",
    "\n",
    "acc_denoising_autoencoder = pipe_denoising_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a denoising autoencoder: %.6f%%.\" % (auroc_denoising_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a denoising autoencoder: %.6f%%.\" % (acc_denoising_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: 1 Dimensional Convolutional Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convolutional_autoencoder = ConvolutionalAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))),\n",
    "                                                     n_epoch=50,\n",
    "                                                     batch_size=100,\n",
    "                                                     encoder_layers=3,\n",
    "                                                     decoder_layers=3,\n",
    "                                                     filters=100,\n",
    "                                                     kernel_size=5,\n",
    "                                                     strides=1,\n",
    "                                                     pool_size=4,\n",
    "                                                     encoding_dim=100,\n",
    "                                                     denoising=None)\n",
    "\n",
    "print(convolutional_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_convolutional_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional_autoencoder),\n",
    "                                                 (\"scaler_classifier\", scaler_classifier),\n",
    "                                                 (\"classifier\", logistic)])\n",
    "\n",
    "pipe_convolutional_autoencoder = pipe_convolutional_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), \n",
    "                                                                    y_train)\n",
    "\n",
    "auroc_convolutional_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                                pipe_convolutional_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))])),\n",
    "                                                average=\"weighted\")\n",
    "\n",
    "acc_convolutional_autoencoder = pipe_convolutional_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a 1 dimensional convolutional autoencoder: %.6f%%.\" % (auroc_convolutional_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a 1 dimensional convolutional autoencoder: %.6f%%.\" % (acc_convolutional_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Sequence to Sequence Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seq2seq_autoencoder = Seq2SeqAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))),\n",
    "                                         n_epoch=50,\n",
    "                                         batch_size=100,\n",
    "                                         encoder_layers=3,\n",
    "                                         decoder_layers=3,\n",
    "                                         n_hidden_units=100,\n",
    "                                         encoding_dim=100,\n",
    "                                         stateful=False,\n",
    "                                         denoising=None)\n",
    "\n",
    "print(seq2seq_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_seq2seq_autoencoder = Pipeline(steps=[(\"autoencoder\", seq2seq_autoencoder),\n",
    "                                           (\"scaler_classifier\", scaler_classifier),\n",
    "                                           (\"classifier\", logistic)])\n",
    "\n",
    "pipe_seq2seq_autoencoder = pipe_seq2seq_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]),\n",
    "                                                        y_train)\n",
    "\n",
    "auroc_seq2seq_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                          pipe_seq2seq_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))])),\n",
    "                                          average=\"weighted\")\n",
    "\n",
    "acc_seq2seq_autoencoder = pipe_seq2seq_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to sequence autoencoder: %.6f%%.\" % (auroc_seq2seq_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to sequence autoencoder: %.6f%%.\" % (acc_seq2seq_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Variational Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoding_dim = 500\n",
    "\n",
    "variational_autoencoder = VariationalAutoencoder(n_feat=X_train.shape[1],\n",
    "                                                 n_epoch=50,\n",
    "                                                 batch_size=100,\n",
    "                                                 encoder_layers=3,\n",
    "                                                 decoder_layers=3,\n",
    "                                                 n_hidden_units=500,\n",
    "                                                 encoding_dim=encoding_dim,\n",
    "                                                 denoising=None)\n",
    "\n",
    "print(variational_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_variational_autoencoder = Pipeline(steps=[(\"autoencoder\", variational_autoencoder),\n",
    "                                               (\"scaler_classifier\", scaler_classifier),\n",
    "                                               (\"classifier\", logistic)])\n",
    "\n",
    "pipe_variational_autoencoder = pipe_variational_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_variational_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                              pipe_variational_autoencoder.predict_proba(X_test), \n",
    "                                              average=\"weighted\")\n",
    "\n",
    "acc_variational_autoencoder = pipe_variational_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to variational autoencoder: %.6f%%.\" % (auroc_variational_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to variational autoencoder: %.6f%%.\" % (acc_variational_autoencoder * 100))\n",
    "\n",
    "if encoding_dim == 2:\n",
    "    test_encoded_df = pd.DataFrame(pipe_variational_autoencoder.named_steps[\"autoencoder\"].encoder.predict(X_test))\n",
    "    test_encoded_df[\"Target\"] = y_test\n",
    "    test_encoded_df.columns.values[0:2] = [\"Encoding_1\", \"Encoding_2\"]\n",
    "\n",
    "    scaler_plot = MinMaxScaler(feature_range=(0.25, 0.75))\n",
    "    scaler_plot = scaler_plot.fit(test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]])\n",
    "    test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]] = scaler_plot.transform(test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]])\n",
    "\n",
    "    cluster_plot = ggplot(test_encoded_df) + \\\n",
    "    geom_point(aes(x=\"Encoding_1\", \n",
    "                   y=\"Encoding_2\", \n",
    "                   fill=\"factor(Target)\"),\n",
    "               size=1,\n",
    "               color = \"black\") + \\\n",
    "    xlab(\"Encoding dimension 1\") + \\\n",
    "    ylab(\"Encoding dimension 2\") + \\\n",
    "    ggtitle(\"Variational autoencoder with 2-dimensional encoding\") + \\\n",
    "    theme_matplotlib()\n",
    "    print(cluster_plot)\n",
    "\n",
    "    n = 15\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "    for i, xi in enumerate(grid_x):\n",
    "        for j, yi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = pipe_variational_autoencoder.named_steps[\"autoencoder\"].generator.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.title(\"Variational autoencoder with 2-dimensional encoding\\nGenerating new images\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: 2 Dimensional Convolutional Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convolutional2D_autoencoder = Convolutional2DAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1),\n",
    "                                                         n_epoch=5,\n",
    "                                                         batch_size=100,\n",
    "                                                         encoder_layers=3,\n",
    "                                                         decoder_layers=3,\n",
    "                                                         filters=25,\n",
    "                                                         kernel_size=5,\n",
    "                                                         strides=1,\n",
    "                                                         pool_size=4,\n",
    "                                                         denoising=None)\n",
    "\n",
    "print(convolutional2D_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_convolutional2D_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional2D_autoencoder),\n",
    "                                                   (\"scaler_classifier\", scaler_classifier),\n",
    "                                                   (\"classifier\", logistic)])\n",
    "\n",
    "pipe_convolutional2D_autoencoder = pipe_convolutional2D_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1]),\n",
    "                                                                        y_train)\n",
    "\n",
    "auroc_convolutional2D_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                                  pipe_convolutional2D_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1])),\n",
    "                                                  average=\"weighted\")\n",
    "\n",
    "acc_convolutional2D_autoencoder = pipe_convolutional2D_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_test.shape[1], 0.5)), int(math.pow(X_test.shape[1], 0.5)), 1]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to 2 dimensional convolutional autoencoder: %.6f%%.\" % (auroc_convolutional2D_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to 2 dimensional convolutional autoencoder: %.6f%%.\" % (acc_convolutional2D_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance: No Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUROC score for the insurance classification task without autoencoders: 92.206261%.\n",
      "The accuracy score for the insurance classification task without autoencoders: 82.333333%.\n"
     ]
    }
   ],
   "source": [
    "claim_risk = pd.read_csv(filepath_or_buffer=\"../R/data/claim_risk.csv\")\n",
    "claim_risk.drop(columns=\"policy.id\", axis=1, inplace=True)\n",
    "claim_risk = np.asarray(claim_risk).ravel()\n",
    "\n",
    "transactions = pd.read_csv(filepath_or_buffer=\"../R/data/transactions.csv\")\n",
    "transactions.drop(columns=\"policy.id\", axis=1, inplace=True)\n",
    "\n",
    "n_policies = 1000\n",
    "n_transaction_types = 3\n",
    "n_time_periods = 4\n",
    "\n",
    "transactions = np.reshape(np.asarray(transactions), (n_policies, n_time_periods * n_transaction_types))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(transactions, claim_risk, test_size=0.3, random_state=666)\n",
    "\n",
    "min_X_train = np.apply_along_axis(func1d=np.min, axis=0, arr=X_train)\n",
    "max_X_train = np.apply_along_axis(func1d=np.max, axis=0, arr=X_train) \n",
    "range_X_train = max_X_train - min_X_train + sys.float_info.epsilon\n",
    "X_train = (X_train - min_X_train) / range_X_train\n",
    "X_test = (X_test - min_X_train) / range_X_train\n",
    "\n",
    "pipe_base = Pipeline(steps=[(\"classifier\", logistic)])\n",
    "\n",
    "pipe_base = pipe_base.fit(X_train, y_train)\n",
    "\n",
    "auroc_base = roc_auc_score(y_true=y_test,\n",
    "                           y_score=pipe_base.predict_proba(X_test)[:, 1], \n",
    "                           average=\"weighted\")\n",
    "    \n",
    "acc_base = pipe_base.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the insurance classification task without autoencoders: %.6f%%.\" % (auroc_base * 100))\n",
    "print(\"The accuracy score for the insurance classification task without autoencoders: %.6f%%.\" % (acc_base * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance: Handcrafted Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUROC score for the insurance classification task with handcrafted features: 93.610635%.\n",
      "The accuracy score for the insurance classification task with handcrafted features: 85.000000%.\n"
     ]
    }
   ],
   "source": [
    "claim_risk = pd.read_csv(filepath_or_buffer=\"../R/data/claim_risk.csv\")\n",
    "claim_risk.drop(columns=\"policy.id\", axis=1, inplace=True)\n",
    "claim_risk = np.asarray(claim_risk).ravel()\n",
    "\n",
    "handcrafted_features = pd.read_csv(filepath_or_buffer=\"../R/data/handcrafted_features.csv\")\n",
    "handcrafted_features = np.asarray(handcrafted_features)\n",
    "\n",
    "n_policies = 1000\n",
    "n_feat = 12\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(handcrafted_features, claim_risk, test_size=0.3, random_state=666)\n",
    "\n",
    "min_X_train = np.apply_along_axis(func1d=np.min, axis=0, arr=X_train)\n",
    "max_X_train = np.apply_along_axis(func1d=np.max, axis=0, arr=X_train) \n",
    "range_X_train = max_X_train - min_X_train + sys.float_info.epsilon\n",
    "X_train = (X_train - min_X_train) / range_X_train\n",
    "X_test = (X_test - min_X_train) / range_X_train\n",
    "\n",
    "pipe_base = Pipeline(steps=[(\"classifier\", logistic)])\n",
    "\n",
    "pipe_base = pipe_base.fit(X_train, y_train)\n",
    "\n",
    "auroc_base = roc_auc_score(y_true=y_test,\n",
    "                           y_score=pipe_base.predict_proba(X_test)[:, 1], \n",
    "                           average=\"weighted\")\n",
    "    \n",
    "acc_base = pipe_base.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the insurance classification task with handcrafted features: %.6f%%.\" % (auroc_base * 100))\n",
    "print(\"The accuracy score for the insurance classification task with handcrafted features: %.6f%%.\" % (acc_base * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance: Vanilla Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                 n_epoch=100,\n",
    "                                 batch_size=50,\n",
    "                                 encoder_layers=1,\n",
    "                                 decoder_layers=1,\n",
    "                                 n_hidden_units=50,\n",
    "                                 encoding_dim=50,\n",
    "                                 denoising=None)\n",
    "\n",
    "print(autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_autoencoder = Pipeline(steps=[(\"autoencoder\", autoencoder),\n",
    "                                   (\"scaler_classifier\", scaler_classifier),\n",
    "                                   (\"classifier\", logistic)])\n",
    "\n",
    "pipe_autoencoder = pipe_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_autoencoder = roc_auc_score(y_true=y_test,\n",
    "                                  y_score=pipe_autoencoder.predict_proba(X_test)[:, 1], \n",
    "                                  average=\"weighted\")\n",
    "    \n",
    "acc_autoencoder = pipe_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the insurance classification task with an autoencoder: %.6f%%.\" % (auroc_autoencoder * 100))\n",
    "print(\"The accuracy score for the insurance classification task with an autoencoder: %.6f%%.\" % (acc_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance: Denoising Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise = 0.10 * np.reshape(np.random.uniform(low=0.0, \n",
    "                                            high=1.0, \n",
    "                                            size=X_train.shape[0] * X_train.shape[1]), \n",
    "                          [X_train.shape[0], X_train.shape[1]])\n",
    "\n",
    "denoising_autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                           n_epoch=100,\n",
    "                                           batch_size=50,\n",
    "                                           encoder_layers=1,\n",
    "                                           decoder_layers=1,\n",
    "                                           n_hidden_units=50,\n",
    "                                           encoding_dim=50,\n",
    "                                           denoising=noise)\n",
    "\n",
    "print(denoising_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_denoising_autoencoder = Pipeline(steps=[(\"autoencoder\", denoising_autoencoder),\n",
    "                                             (\"scaler_classifier\", scaler_classifier),\n",
    "                                             (\"classifier\", logistic)])\n",
    "\n",
    "pipe_denoising_autoencoder = pipe_denoising_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_denoising_autoencoder = roc_auc_score(y_true=y_test,\n",
    "                                            y_score=pipe_denoising_autoencoder.predict_proba(X_test)[:, 1], \n",
    "                                            average=\"weighted\")\n",
    "    \n",
    "acc_denoising_autoencoder = pipe_denoising_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the insurance classification task with a denoising autoencoder: %.6f%%.\" % (auroc_denoising_autoencoder * 100))\n",
    "print(\"The accuracy score for the insurance classification task with a denoising autoencoder: %.6f%%.\" % (acc_denoising_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance: Sequence to Sequence Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_15 (CuDNNLSTM)    (None, 4, 1000)           4020000   \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 4, 1000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 4, 1000)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_16 (CuDNNLSTM)    (None, 100)               440800    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 4, 100)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_17 (CuDNNLSTM)    (None, 4, 1000)           4408000   \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 4, 1000)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)    (None, 4, 3)              12060     \n",
      "=================================================================\n",
      "Total params: 8,880,860\n",
      "Trainable params: 8,880,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 489 samples, validate on 211 samples\n",
      "Epoch 1/25\n",
      "489/489 [==============================] - 8s 17ms/step - loss: 0.0148 - val_loss: 0.0143\n",
      "Epoch 2/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0126 - val_loss: 0.0134\n",
      "Epoch 3/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 4/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0117 - val_loss: 0.0129\n",
      "Epoch 5/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0114 - val_loss: 0.0122\n",
      "Epoch 6/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 7/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 8/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 9/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 10/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 11/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 12/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0090 - val_loss: 0.0092\n",
      "Epoch 13/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0083 - val_loss: 0.0096\n",
      "Epoch 14/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 15/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0082 - val_loss: 0.0087\n",
      "Epoch 16/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0080 - val_loss: 0.0091\n",
      "Epoch 17/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0080 - val_loss: 0.0084\n",
      "Epoch 18/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0075 - val_loss: 0.0080\n",
      "Epoch 19/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 20/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 21/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0068 - val_loss: 0.0088\n",
      "Epoch 22/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0070 - val_loss: 0.0080\n",
      "Epoch 23/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0071 - val_loss: 0.0084\n",
      "Epoch 24/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 25/25\n",
      "489/489 [==============================] - 2s 4ms/step - loss: 0.0065 - val_loss: 0.0071\n",
      "The AUROC score for the insurance classification task with a sequence to sequence autoencoder: 91.332547%.\n",
      "The accuracy score for the insurance classification task with a sequence to sequence autoencoder: 84.000000%.\n"
     ]
    }
   ],
   "source": [
    "transactions = np.reshape(np.asarray(transactions), (n_policies, n_time_periods, n_transaction_types))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(transactions, claim_risk, test_size=0.3, random_state=666)\n",
    "\n",
    "min_X_train = np.apply_along_axis(func1d=np.min, axis=0, arr=X_train)\n",
    "max_X_train = np.apply_along_axis(func1d=np.max, axis=0, arr=X_train) \n",
    "range_X_train = max_X_train - min_X_train + sys.float_info.epsilon\n",
    "X_train = (X_train - min_X_train) / range_X_train\n",
    "X_test = (X_test - min_X_train) / range_X_train\n",
    "\n",
    "seq2seq_autoencoder = Seq2SeqAutoencoder(input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                                         n_epoch=25,\n",
    "                                         batch_size=10,\n",
    "                                         encoder_layers=1,\n",
    "                                         decoder_layers=1,\n",
    "                                         n_hidden_units=1000,\n",
    "                                         encoding_dim=100,\n",
    "                                         stateful=False,\n",
    "                                         denoising=None)\n",
    "\n",
    "print(seq2seq_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_seq2seq_autoencoder = Pipeline(steps=[(\"autoencoder\", seq2seq_autoencoder),\n",
    "                                           (\"scaler_classifier\", scaler_classifier),\n",
    "                                           (\"classifier\", logistic)])\n",
    "\n",
    "pipe_seq2seq_autoencoder = pipe_seq2seq_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_seq2seq_autoencoder = roc_auc_score(y_test, \n",
    "                                          pipe_seq2seq_autoencoder.predict_proba(X_test)[:, 1],\n",
    "                                          average=\"weighted\")\n",
    "\n",
    "acc_seq2seq_autoencoder = pipe_seq2seq_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the insurance classification task with a sequence to sequence autoencoder: %.6f%%.\" % (auroc_seq2seq_autoencoder * 100))\n",
    "print(\"The accuracy score for the insurance classification task with a sequence to sequence autoencoder: %.6f%%.\" % (acc_seq2seq_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Hamaad Musharaf Shah.\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, local, convolutional, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from autoencoders_keras.loss_history import LossHistory\n",
    "\n",
    "class ConvolutionalAutoencoder(BaseEstimator, \n",
    "                               TransformerMixin):\n",
    "    def __init__(self, \n",
    "                 input_shape=None,\n",
    "                 n_epoch=None,\n",
    "                 batch_size=None,\n",
    "                 encoder_layers=None,\n",
    "                 decoder_layers=None,\n",
    "                 filters=None,\n",
    "                 kernel_size=None,\n",
    "                 strides=None,\n",
    "                 pool_size=None,\n",
    "                 denoising=None):\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "        \n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "        \n",
    "        loss_history = LossHistory()\n",
    "        self.callbacks_list = [loss_history]\n",
    "        \n",
    "        for i in range(self.encoder_layers):\n",
    "            if i == 0:\n",
    "                with tensorflow.device(\"/gpu:0\"):\n",
    "                    self.input_data = Input(shape=self.input_shape)\n",
    "                    self.encoded = BatchNormalization()(self.input_data)\n",
    "                    self.encoded = keras.layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, strides=self.strides, activation=\"elu\", padding=\"same\")(self.encoded)\n",
    "                    self.encoded = Dropout(rate=0.5)(self.encoded)\n",
    "            elif i > 0 and i < self.encoder_layers - 1:\n",
    "                with tensorflow.device(\"/gpu:0\"):\n",
    "                    self.encoded = BatchNormalization()(self.encoded)\n",
    "                    self.encoded = keras.layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, strides=self.strides, activation=\"elu\", padding=\"same\")(self.encoded)\n",
    "                    self.encoded = Dropout(rate=0.5)(self.encoded)\n",
    "            elif i == self.encoder_layers - 1:\n",
    "                with tensorflow.device(\"/gpu:0\"):\n",
    "                    self.encoded = BatchNormalization()(self.encoded)\n",
    "                    self.encoded = keras.layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, strides=self.strides, activation=\"elu\", padding=\"same\")(self.encoded)\n",
    "                    self.encoded = Dropout(rate=0.5)(self.encoded)\n",
    "\n",
    "        with tensorflow.device(\"/gpu:0\"):\n",
    "            self.encoded = keras.layers.MaxPooling1D(strides=self.pool_size, padding=\"valid\")(self.encoded)\n",
    "            self.decoded = BatchNormalization()(self.encoded)\n",
    "\n",
    "        for i in range(self.decoder_layers):\n",
    "            if i == 0:\n",
    "                with tensorflow.device(\"/gpu:0\"):\n",
    "                    self.decoded = keras.layers.UpSampling1D(size=self.pool_size)(self.decoded)\n",
    "                    self.decoded = Dropout(rate=0.5)(self.decoded)\n",
    "            elif i > 0 and i < self.decoder_layers - 1:\n",
    "                with tensorflow.device(\"/gpu:0\"):\n",
    "                    self.decoded = BatchNormalization()(self.decoded)\n",
    "                    self.decoded = keras.layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, strides=self.strides, activation=\"elu\", padding=\"same\")(self.decoded)\n",
    "                    self.decoded = Dropout(rate=0.5)(self.decoded)\n",
    "            elif i == self.decoder_layers - 1:\n",
    "                with tensorflow.device(\"/gpu:0\"):\n",
    "                    self.decoded = BatchNormalization()(self.decoded)\n",
    "                    self.decoded = keras.layers.Conv1D(filters=self.filters, kernel_size=self.kernel_size, strides=self.strides, activation=\"elu\", padding=\"same\")(self.decoded)\n",
    "                    self.decoded = Dropout(rate=0.5)(self.decoded)\n",
    "        \n",
    "        with tensorflow.device(\"/gpu:0\"):\n",
    "            # 3D tensor with shape: (batch_size, new_steps, filters).\n",
    "            # Remember think of this as a 2D-Lattice per observation.\n",
    "            # Rows represent time and columns represent some quantities of interest that evolve over time.\n",
    "            self.decoded = BatchNormalization()(self.decoded)\n",
    "            self.decoded = keras.layers.Conv1D(filters=self.input_shape[1], kernel_size=self.kernel_size, strides=self.strides, activation=\"sigmoid\", padding=\"same\")(self.decoded)\n",
    "\n",
    "            self.autoencoder = Model(self.input_data, self.decoded)\n",
    "            self.autoencoder.compile(optimizer=keras.optimizers.Adam(),\n",
    "                                     loss=\"mean_squared_error\")\n",
    "            \n",
    "    def fit(self,\n",
    "            X,\n",
    "            y=None):\n",
    "        with tensorflow.device(\"/gpu:0\"):\n",
    "            keras.backend.get_session().run(tensorflow.global_variables_initializer())\n",
    "            self.autoencoder.fit(X if self.denoising is None else X + self.denoising, X,\n",
    "                                 validation_split=0.3,\n",
    "                                 epochs=self.n_epoch,\n",
    "                                 batch_size=self.batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 callbacks=self.callbacks_list, \n",
    "                                 verbose=1)\n",
    "\n",
    "            self.encoder = Model(self.input_data, self.encoded)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self,\n",
    "                  X):\n",
    "        with tensorflow.device(\"/gpu:0\"):\n",
    "            out = np.reshape(self.encoder.predict(X), (X.shape[0], self.filters * int(X.shape[1] / self.pool_size)))\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 4, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 4, 3)              12        \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 4, 250)            3250      \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 4, 250)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 4, 250)            1000      \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 4, 250)            250250    \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 4, 250)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 2, 250)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_140 (Bat (None, 2, 250)            1000      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_24 (UpSampling (None, 4, 250)            0         \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 4, 250)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_141 (Bat (None, 4, 250)            1000      \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 4, 250)            250250    \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 4, 250)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_142 (Bat (None, 4, 250)            1000      \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 4, 3)              3003      \n",
      "=================================================================\n",
      "Total params: 510,765\n",
      "Trainable params: 508,759\n",
      "Non-trainable params: 2,006\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "transactions = np.reshape(np.asarray(transactions), (n_policies, n_time_periods, n_transaction_types))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(transactions, claim_risk, test_size=0.3, random_state=666)\n",
    "print(X_train.shape)\n",
    "min_X_train = np.apply_along_axis(func1d=np.min, axis=0, arr=X_train)\n",
    "max_X_train = np.apply_along_axis(func1d=np.max, axis=0, arr=X_train) \n",
    "range_X_train = max_X_train - min_X_train + sys.float_info.epsilon\n",
    "X_train = (X_train - min_X_train) / range_X_train\n",
    "X_test = (X_test - min_X_train) / range_X_train\n",
    "\n",
    "convolutional_autoencoder = ConvolutionalAutoencoder(input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                                                     n_epoch=250,\n",
    "                                                     batch_size=10,\n",
    "                                                     encoder_layers=2,\n",
    "                                                     decoder_layers=2,\n",
    "                                                     filters=250,\n",
    "                                                     kernel_size=4,\n",
    "                                                     strides=1,\n",
    "                                                     pool_size=2,\n",
    "                                                     denoising=None)\n",
    "\n",
    "print(convolutional_autoencoder.autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_142/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_142/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_141/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_141/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_140/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_140/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training_23/SGD/gradients/batch_normalization_139/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_139/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "Train on 489 samples, validate on 211 samples\n",
      "Epoch 1/250\n",
      "489/489 [==============================] - 7s 14ms/step - loss: 0.2353 - val_loss: 0.1613\n",
      "Epoch 2/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.2227 - val_loss: 0.1378\n",
      "Epoch 3/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.2096 - val_loss: 0.1178\n",
      "Epoch 4/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1985 - val_loss: 0.1006\n",
      "Epoch 5/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1927 - val_loss: 0.0883\n",
      "Epoch 6/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1784 - val_loss: 0.0781\n",
      "Epoch 7/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1695 - val_loss: 0.0700\n",
      "Epoch 8/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1603 - val_loss: 0.0641\n",
      "Epoch 9/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1517 - val_loss: 0.0591\n",
      "Epoch 10/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1433 - val_loss: 0.0562\n",
      "Epoch 11/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1355 - val_loss: 0.0528\n",
      "Epoch 12/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1274 - val_loss: 0.0487\n",
      "Epoch 13/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1183 - val_loss: 0.0443\n",
      "Epoch 14/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1137 - val_loss: 0.0408\n",
      "Epoch 15/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1057 - val_loss: 0.0378\n",
      "Epoch 16/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.1003 - val_loss: 0.0344\n",
      "Epoch 17/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0914 - val_loss: 0.0315\n",
      "Epoch 18/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0855 - val_loss: 0.0289\n",
      "Epoch 19/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0817 - val_loss: 0.0268\n",
      "Epoch 20/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0760 - val_loss: 0.0242\n",
      "Epoch 21/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0708 - val_loss: 0.0219\n",
      "Epoch 22/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0655 - val_loss: 0.0204\n",
      "Epoch 23/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0619 - val_loss: 0.0191\n",
      "Epoch 24/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0569 - val_loss: 0.0176\n",
      "Epoch 25/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0549 - val_loss: 0.0163\n",
      "Epoch 26/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0524 - val_loss: 0.0151\n",
      "Epoch 27/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0497 - val_loss: 0.0142\n",
      "Epoch 28/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0453 - val_loss: 0.0135\n",
      "Epoch 29/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0439 - val_loss: 0.0125\n",
      "Epoch 30/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0425 - val_loss: 0.0118\n",
      "Epoch 31/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0387 - val_loss: 0.0112\n",
      "Epoch 32/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0372 - val_loss: 0.0108\n",
      "Epoch 33/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0351 - val_loss: 0.0105\n",
      "Epoch 34/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0326 - val_loss: 0.0102\n",
      "Epoch 35/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0330 - val_loss: 0.0099\n",
      "Epoch 36/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0304 - val_loss: 0.0096\n",
      "Epoch 37/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0303 - val_loss: 0.0092\n",
      "Epoch 38/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0287 - val_loss: 0.0091\n",
      "Epoch 39/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0281 - val_loss: 0.0090\n",
      "Epoch 40/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0271 - val_loss: 0.0089\n",
      "Epoch 41/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0263 - val_loss: 0.0087\n",
      "Epoch 42/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0243 - val_loss: 0.0086\n",
      "Epoch 43/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0243 - val_loss: 0.0084\n",
      "Epoch 44/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0230 - val_loss: 0.0083\n",
      "Epoch 45/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0236 - val_loss: 0.0082\n",
      "Epoch 46/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0215 - val_loss: 0.0082\n",
      "Epoch 47/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0228 - val_loss: 0.0081\n",
      "Epoch 48/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0221 - val_loss: 0.0081\n",
      "Epoch 49/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0211 - val_loss: 0.0081\n",
      "Epoch 50/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0199 - val_loss: 0.0080\n",
      "Epoch 51/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0199 - val_loss: 0.0080\n",
      "Epoch 52/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0189 - val_loss: 0.0079\n",
      "Epoch 53/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0186 - val_loss: 0.0080\n",
      "Epoch 54/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0181 - val_loss: 0.0079\n",
      "Epoch 55/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0180 - val_loss: 0.0078\n",
      "Epoch 56/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0176 - val_loss: 0.0078\n",
      "Epoch 57/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0178 - val_loss: 0.0078\n",
      "Epoch 58/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0171 - val_loss: 0.0077\n",
      "Epoch 59/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0174 - val_loss: 0.0075\n",
      "Epoch 60/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0169 - val_loss: 0.0075\n",
      "Epoch 61/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0160 - val_loss: 0.0076\n",
      "Epoch 62/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0169 - val_loss: 0.0076\n",
      "Epoch 63/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0171 - val_loss: 0.0076\n",
      "Epoch 64/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0159 - val_loss: 0.0076\n",
      "Epoch 65/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0162 - val_loss: 0.0075\n",
      "Epoch 66/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0161 - val_loss: 0.0073\n",
      "Epoch 67/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0150 - val_loss: 0.0073\n",
      "Epoch 68/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0149 - val_loss: 0.0073\n",
      "Epoch 69/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0151 - val_loss: 0.0073\n",
      "Epoch 70/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0156 - val_loss: 0.0073\n",
      "Epoch 71/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0158 - val_loss: 0.0074\n",
      "Epoch 72/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0151 - val_loss: 0.0073\n",
      "Epoch 73/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0150 - val_loss: 0.0074\n",
      "Epoch 74/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0148 - val_loss: 0.0072\n",
      "Epoch 75/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0148 - val_loss: 0.0072\n",
      "Epoch 76/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0146 - val_loss: 0.0072\n",
      "Epoch 77/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0138 - val_loss: 0.0070\n",
      "Epoch 78/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0153 - val_loss: 0.0070\n",
      "Epoch 79/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0145 - val_loss: 0.0071\n",
      "Epoch 80/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0144 - val_loss: 0.0070\n",
      "Epoch 81/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0143 - val_loss: 0.0070\n",
      "Epoch 82/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0135 - val_loss: 0.0069\n",
      "Epoch 83/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0142 - val_loss: 0.0069\n",
      "Epoch 84/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0134 - val_loss: 0.0069\n",
      "Epoch 85/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0134 - val_loss: 0.0068\n",
      "Epoch 86/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0135 - val_loss: 0.0068\n",
      "Epoch 87/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0132 - val_loss: 0.0068\n",
      "Epoch 88/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0140 - val_loss: 0.0067\n",
      "Epoch 89/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0135 - val_loss: 0.0067\n",
      "Epoch 90/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0133 - val_loss: 0.0068\n",
      "Epoch 91/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0135 - val_loss: 0.0068\n",
      "Epoch 92/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0131 - val_loss: 0.0067\n",
      "Epoch 93/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0134 - val_loss: 0.0067\n",
      "Epoch 94/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0128 - val_loss: 0.0066\n",
      "Epoch 95/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0132 - val_loss: 0.0064\n",
      "Epoch 96/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0130 - val_loss: 0.0065\n",
      "Epoch 97/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0131 - val_loss: 0.0066\n",
      "Epoch 98/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0134 - val_loss: 0.0065\n",
      "Epoch 99/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0136 - val_loss: 0.0065\n",
      "Epoch 100/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0124 - val_loss: 0.0064\n",
      "Epoch 101/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0123 - val_loss: 0.0065\n",
      "Epoch 102/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0120 - val_loss: 0.0063\n",
      "Epoch 103/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0130 - val_loss: 0.0063\n",
      "Epoch 104/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0130 - val_loss: 0.0063\n",
      "Epoch 105/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0128 - val_loss: 0.0064\n",
      "Epoch 106/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0125 - val_loss: 0.0063\n",
      "Epoch 107/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0122 - val_loss: 0.0062\n",
      "Epoch 108/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0124 - val_loss: 0.0062\n",
      "Epoch 109/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0126 - val_loss: 0.0062\n",
      "Epoch 110/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0126 - val_loss: 0.0063\n",
      "Epoch 111/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0122 - val_loss: 0.0063\n",
      "Epoch 112/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0119 - val_loss: 0.0062\n",
      "Epoch 113/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0126 - val_loss: 0.0062\n",
      "Epoch 114/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0119 - val_loss: 0.0062\n",
      "Epoch 115/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0121 - val_loss: 0.0061\n",
      "Epoch 116/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0123 - val_loss: 0.0061\n",
      "Epoch 117/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0122 - val_loss: 0.0062\n",
      "Epoch 118/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0123 - val_loss: 0.0061\n",
      "Epoch 119/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0120 - val_loss: 0.0061\n",
      "Epoch 120/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0122 - val_loss: 0.0060\n",
      "Epoch 121/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0113 - val_loss: 0.0060\n",
      "Epoch 122/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0129 - val_loss: 0.0060\n",
      "Epoch 123/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0115 - val_loss: 0.0059\n",
      "Epoch 124/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0121 - val_loss: 0.0059\n",
      "Epoch 125/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0112 - val_loss: 0.0059\n",
      "Epoch 126/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0125 - val_loss: 0.0059\n",
      "Epoch 127/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0118 - val_loss: 0.0059\n",
      "Epoch 128/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0121 - val_loss: 0.0059\n",
      "Epoch 129/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0120 - val_loss: 0.0059\n",
      "Epoch 130/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0122 - val_loss: 0.0059\n",
      "Epoch 131/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0109 - val_loss: 0.0058\n",
      "Epoch 132/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0117 - val_loss: 0.0058\n",
      "Epoch 133/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0113 - val_loss: 0.0057\n",
      "Epoch 134/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0118 - val_loss: 0.0056\n",
      "Epoch 135/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0116 - val_loss: 0.0057\n",
      "Epoch 136/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0110 - val_loss: 0.0057\n",
      "Epoch 137/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0118 - val_loss: 0.0056\n",
      "Epoch 138/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0114 - val_loss: 0.0056\n",
      "Epoch 139/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0119 - val_loss: 0.0057\n",
      "Epoch 140/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0113 - val_loss: 0.0056\n",
      "Epoch 141/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0116 - val_loss: 0.0056\n",
      "Epoch 142/250\n",
      "489/489 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0056\n",
      "Epoch 143/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0113 - val_loss: 0.0055\n",
      "Epoch 144/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0111 - val_loss: 0.0055\n",
      "Epoch 145/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0110 - val_loss: 0.0055\n",
      "Epoch 146/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0111 - val_loss: 0.0054\n",
      "Epoch 147/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0114 - val_loss: 0.0054\n",
      "Epoch 148/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0118 - val_loss: 0.0054\n",
      "Epoch 149/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0114 - val_loss: 0.0054\n",
      "Epoch 150/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0108 - val_loss: 0.0055\n",
      "Epoch 151/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0108 - val_loss: 0.0054\n",
      "Epoch 152/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0117 - val_loss: 0.0054\n",
      "Epoch 153/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0112 - val_loss: 0.0054\n",
      "Epoch 154/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0106 - val_loss: 0.0054\n",
      "Epoch 155/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0111 - val_loss: 0.0054\n",
      "Epoch 156/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0054\n",
      "Epoch 157/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0105 - val_loss: 0.0053\n",
      "Epoch 158/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0114 - val_loss: 0.0053\n",
      "Epoch 159/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0053\n",
      "Epoch 160/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0113 - val_loss: 0.0053\n",
      "Epoch 161/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0111 - val_loss: 0.0052\n",
      "Epoch 162/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0108 - val_loss: 0.0051\n",
      "Epoch 163/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0114 - val_loss: 0.0051\n",
      "Epoch 164/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0112 - val_loss: 0.0051\n",
      "Epoch 165/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0108 - val_loss: 0.0051\n",
      "Epoch 166/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0105 - val_loss: 0.0051\n",
      "Epoch 167/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0111 - val_loss: 0.0051\n",
      "Epoch 168/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0114 - val_loss: 0.0050\n",
      "Epoch 169/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0108 - val_loss: 0.0051\n",
      "Epoch 170/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0109 - val_loss: 0.0051\n",
      "Epoch 171/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0104 - val_loss: 0.0051\n",
      "Epoch 172/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0051\n",
      "Epoch 173/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0112 - val_loss: 0.0051\n",
      "Epoch 174/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0109 - val_loss: 0.0051\n",
      "Epoch 175/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0108 - val_loss: 0.0050\n",
      "Epoch 176/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0112 - val_loss: 0.0051\n",
      "Epoch 177/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0106 - val_loss: 0.0050\n",
      "Epoch 178/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0050\n",
      "Epoch 179/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0113 - val_loss: 0.0050\n",
      "Epoch 180/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0106 - val_loss: 0.0050\n",
      "Epoch 181/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0104 - val_loss: 0.0050\n",
      "Epoch 182/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0102 - val_loss: 0.0049\n",
      "Epoch 183/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0049\n",
      "Epoch 184/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0110 - val_loss: 0.0050\n",
      "Epoch 185/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0109 - val_loss: 0.0050\n",
      "Epoch 186/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0050\n",
      "Epoch 187/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0049\n",
      "Epoch 188/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0103 - val_loss: 0.0049\n",
      "Epoch 189/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0103 - val_loss: 0.0049\n",
      "Epoch 190/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0103 - val_loss: 0.0050\n",
      "Epoch 191/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0104 - val_loss: 0.0049\n",
      "Epoch 192/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0105 - val_loss: 0.0049\n",
      "Epoch 193/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0048\n",
      "Epoch 194/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0048\n",
      "Epoch 195/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0109 - val_loss: 0.0049\n",
      "Epoch 196/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0102 - val_loss: 0.0049\n",
      "Epoch 197/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0049\n",
      "Epoch 198/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0099 - val_loss: 0.0049\n",
      "Epoch 199/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0106 - val_loss: 0.0049\n",
      "Epoch 200/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0108 - val_loss: 0.0048\n",
      "Epoch 201/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0102 - val_loss: 0.0048\n",
      "Epoch 202/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0102 - val_loss: 0.0048\n",
      "Epoch 203/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0048\n",
      "Epoch 204/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0106 - val_loss: 0.0049\n",
      "Epoch 205/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0106 - val_loss: 0.0048\n",
      "Epoch 206/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0047\n",
      "Epoch 207/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0097 - val_loss: 0.0047\n",
      "Epoch 208/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0047\n",
      "Epoch 209/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0097 - val_loss: 0.0047\n",
      "Epoch 210/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0108 - val_loss: 0.0047\n",
      "Epoch 211/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0102 - val_loss: 0.0047\n",
      "Epoch 212/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0100 - val_loss: 0.0047\n",
      "Epoch 213/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0047\n",
      "Epoch 214/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0107 - val_loss: 0.0048\n",
      "Epoch 215/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0102 - val_loss: 0.0047\n",
      "Epoch 216/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0106 - val_loss: 0.0047\n",
      "Epoch 217/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0046\n",
      "Epoch 218/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0100 - val_loss: 0.0047\n",
      "Epoch 219/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0047\n",
      "Epoch 220/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0100 - val_loss: 0.0047\n",
      "Epoch 221/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0094 - val_loss: 0.0046\n",
      "Epoch 222/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0096 - val_loss: 0.0046\n",
      "Epoch 223/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0046\n",
      "Epoch 224/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0099 - val_loss: 0.0046\n",
      "Epoch 225/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0097 - val_loss: 0.0046\n",
      "Epoch 226/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0097 - val_loss: 0.0046\n",
      "Epoch 227/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0096 - val_loss: 0.0045\n",
      "Epoch 228/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0045\n",
      "Epoch 229/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0102 - val_loss: 0.0045\n",
      "Epoch 230/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0046\n",
      "Epoch 231/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0099 - val_loss: 0.0046\n",
      "Epoch 232/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0100 - val_loss: 0.0046\n",
      "Epoch 233/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0099 - val_loss: 0.0046\n",
      "Epoch 234/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0046\n",
      "Epoch 235/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0100 - val_loss: 0.0045\n",
      "Epoch 236/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0099 - val_loss: 0.0045\n",
      "Epoch 237/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0099 - val_loss: 0.0045\n",
      "Epoch 238/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 239/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 240/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 241/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0044\n",
      "Epoch 242/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0044\n",
      "Epoch 243/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0043\n",
      "Epoch 244/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 245/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0093 - val_loss: 0.0044\n",
      "Epoch 246/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0101 - val_loss: 0.0045\n",
      "Epoch 247/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 248/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0094 - val_loss: 0.0044\n",
      "Epoch 249/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 250/250\n",
      "489/489 [==============================] - 1s 1ms/step - loss: 0.0094 - val_loss: 0.0044\n",
      "The AUROC score for the insurance classification task with a 1 dimensional convolutional autoencoder: 93.026372%.\n",
      "The accuracy score for the insurance classification task with a 1 dimensional convolutional autoencoder: 85.666667%.\n"
     ]
    }
   ],
   "source": [
    "pipe_convolutional_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional_autoencoder),\n",
    "                                                 (\"scaler_classifier\", scaler_classifier),\n",
    "                                                 (\"classifier\", logistic)])\n",
    "\n",
    "pipe_convolutional_autoencoder = pipe_convolutional_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_convolutional_autoencoder = roc_auc_score(y_test, \n",
    "                                                pipe_convolutional_autoencoder.predict_proba(X_test)[:, 1],\n",
    "                                                average=\"weighted\")\n",
    "\n",
    "acc_convolutional_autoencoder = pipe_convolutional_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the insurance classification task with a 1 dimensional convolutional autoencoder: %.6f%%.\" % (auroc_convolutional_autoencoder * 100))\n",
    "print(\"The accuracy score for the insurance classification task with a 1 dimensional convolutional autoencoder: %.6f%%.\" % (acc_convolutional_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "\n",
    "1. Goodfellow, I., Bengio, Y. and Courville A. (2016). Deep Learning (MIT Press).\n",
    "2. Geron, A. (2017). Hands-On Machine Learning with Scikit-Learn & Tensorflow (O'Reilly).\n",
    "3. http://scikit-learn.org/stable/#\n",
    "4. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "5. https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano\n",
    "6. https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "7. https://keras.io"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
