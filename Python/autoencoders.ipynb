{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "---\n",
    "\n",
    "We will explore the use of autoencoders for automatic feature engineering. The idea is to automatically learn a set of features from raw data that can be useful in supervised learning tasks such as in computer vision and insurance.\n",
    "\n",
    "## Computer Vision\n",
    "---\n",
    "\n",
    "We will use the MNIST dataset for this purpose where the raw data is a 2 dimensional tensor of pixel intensities per image. The image is our unit of analysis: We will predict the probability of each class for each image. This is a multiclass classification task and we will use the one against all AUROC score and accuracy score to assess model performance on the test fold.\n",
    "\n",
    "## Insurance\n",
    "---\n",
    "\n",
    "We will use a dataset from the R package \"insuranceData\" where the raw data is a 2 dimensional tensor of historical policy level information per policy-period combination. The policy-period combination is our unit of analysis: We will predict the probability of loss for each policy-period combination. This is a binary class classification task and we will use the AUROC score and accuracy score to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/samson/anaconda3/envs/autoencoders/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9822157098076685420\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1810374656\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13296979930656387060\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:c2:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Author: Hamaad Musharaf Shah.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import keras\n",
    "from keras import backend as bkend\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Flatten, convolutional, pooling\n",
    "from keras import metrics\n",
    "\n",
    "from autoencoders_keras.get_session import get_session\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "KTF.set_session(get_session(gpu_fraction=0.75, allow_soft_placement=True, log_device_placement=False))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from autoencoders_keras.vanilla_autoencoder import VanillaAutoencoder\n",
    "from autoencoders_keras.convolutional_autoencoder import ConvolutionalAutoencoder\n",
    "from autoencoders_keras.convolutional2D_autoencoder import Convolutional2DAutoencoder\n",
    "from autoencoders_keras.seq2seq_autoencoder import Seq2SeqAutoencoder\n",
    "from autoencoders_keras.variational_autoencoder import VariationalAutoencoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "importlib.reload(bkend)\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "mnist = mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = mnist\n",
    "X_train = np.reshape(X_train, [X_train.shape[0], X_train.shape[1] * X_train.shape[1]])\n",
    "X_test = np.reshape(X_test, [X_test.shape[0], X_test.shape[1] * X_test.shape[1]])\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "---\n",
    "\n",
    "We will use the Python machine learning library scikit-learn for data transformation and the classification task. Note that we will code the autoencoders as scikit-learn transformers such that they can be readily used by scikit-learn pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler_classifier = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "logistic = linear_model.LogisticRegression(random_state=666)\n",
    "lb = LabelBinarizer()\n",
    "lb = lb.fit(y_train.reshape(y_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: No Autoencoders\n",
    "---\n",
    "\n",
    "We run the MNIST dataset without using an autoencoder. The 2 dimensional tensor of pixel intensities per image for MNIST images are of dimension $\\mathbb{R}^{28 \\times 28}$. We reshape them as a 1 dimensional tensor of dimension $\\mathbb{R}^{784}$ per image. Therefore we have 784, i.e., $28 \\times 28 = 784$, features for this supervised learning task per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_base = Pipeline(steps=[(\"scaler_classifier\", scaler_classifier),\n",
    "                            (\"classifier\", logistic)])\n",
    "pipe_base = pipe_base.fit(X_train, y_train)\n",
    "\n",
    "auroc_base = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)),\n",
    "                           pipe_base.predict_proba(X_test), \n",
    "                           average=\"weighted\")\n",
    "\n",
    "acc_base = pipe_base.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task without autoencoders: %.6f%%.\" % (auroc_base * 100))\n",
    "print(\"The accuracy score for the MNIST classification task without autoencoders: %.6f%%.\" % (acc_base * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Vanilla Autoencoders\n",
    "---\n",
    "\n",
    "An autoencoder is an unsupervised learning technique where the objective is to learn a set of features that can be used to reconstruct the input data.\n",
    "\n",
    "Our input data is $X \\in \\mathbb{R}^{N \\times 784}$. An encoder function $E$ maps this to a set of $K$ features such that $E: \\mathbb{R}^{N \\times 784} \\rightarrow \\mathbb{R}^{N \\times K}$. A decoder function $D$ uses the set of $K$ features to reconstruct the input data such that $D: \\mathbb{R}^{N \\times K} \\rightarrow \\mathbb{R}^{N \\times 784}$. \n",
    "    \n",
    "Lets denote the reconstructed data as $\\tilde{X} = D(E(X))$. The goal is to learn the encoding and decoding functions such that we minimize the difference between the input data and the reconstructed data. An example for an objective function for this task can be the Mean Squared Error (MSE) such that $\\frac{1}{N}||\\tilde{X} - X||^{2}_{2}$. \n",
    "    \n",
    "We learn the encoding and decoding functions by minimizing the MSE using the parameters that define the encoding and decoding functions: The gradient of the MSE with respect to the parameters are calculated using the chain rule, i.e., backpropagation, and used to update the parameters via an optimization algorithm such as Stochastic Gradient Descent (SGD). \n",
    "\n",
    "Lets assume we have a single layer autoencoder using the Exponential Linear Unit (ELU) activation function, batch normalization, dropout and the Adaptive Moment (Adam) optimization algorithm. $B$ is the batch size, $K$ is the number of features.\n",
    "\n",
    "* **Exponential Linear Unit:** The activation function is smooth everywhere and avoids the vanishing gradient problem as the output takes on negative values when the input is negative. $\\alpha$ is taken to be $1.0$.\n",
    "\n",
    "    \\begin{align}\n",
    "    H_{\\alpha}(z) &= \n",
    "    \\begin{cases}\n",
    "    \\alpha\\left(\\exp(z) - 1\\right) &\\text{ if } z < 0 \\\\\n",
    "    z &\\text{ if } z \\geq 0\n",
    "    \\end{cases} \\\\\n",
    "    \\frac{dH_{\\alpha}(z)}{dz} &= \n",
    "    \\begin{cases}\n",
    "    \\alpha\\left(\\exp(z)\\right) &\\text{ if } z < 0 \\\\\n",
    "    1 &\\text{ if } z \\geq 0\n",
    "    \\end{cases} \n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "* **Batch Normalization:** The idea is to transform the inputs into a hidden layer's activation functions. We standardize or normalize first using the mean and variance parameters on a per feature basis and then learn a set of scaling and shifting parameters on a per feature basis that transforms the data. The following equations describe this layer succintly: The parameters we learn in this layer are $\\left(\\mu_{j}, \\sigma_{j}^2, \\beta_{j}, \\gamma_{j}\\right) \\hspace{0.1cm} \\forall j \\in \\{1, \\dots, K\\}$.\n",
    "\n",
    "    \\begin{align}\n",
    "    \\mu_{j} &= \\frac{1}{B} \\sum_{i=1}^{B} X_{i,j} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\sigma_{j}^2 &= \\frac{1}{B} \\sum_{i=1}^{B} \\left(X_{i,j} - \\mu_{j}\\right)^2 \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\hat{X}_{:,j} &= \\frac{X_{:,j} - \\mu_{j}}{\\sqrt{\\sigma_{j}^2 + \\epsilon}} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    Z_{:,j} &= \\gamma_{j}\\hat{X}_{:,j} + \\beta_{j} \\hspace{0.1cm} &\\forall j \\in \\{1, \\dots, K\\} \\\\\n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "* **Dropout:** This regularization technique simply drops the outputs from input and hidden units with a certain probability say $50\\%$. \n",
    "\n",
    "\n",
    "* **Adam Optimization Algorithm:** This adaptive algorithm combines ideas from the Momentum and RMSProp optimization algorithms. The goal is to have some memory of past gradients which can guide future parameters updates. The following equations for the algorithm succintly describe this method assuming $\\theta$ is our set of parameters to be learnt and $\\eta$ is the learning rate.\n",
    "\n",
    "    \\begin{align}\n",
    "    m &\\leftarrow \\beta_{1}m + \\left[\\left(1 - \\beta_{1}\\right)\\left(\\nabla_{\\theta}\\text{MSE}\\right)\\right] \\\\\n",
    "    s &\\leftarrow \\beta_{2}s + \\left[\\left(1 - \\beta_{2}\\right)\\left(\\nabla_{\\theta}\\text{MSE} \\otimes \\nabla_{\\theta}\\text{MSE} \\right)\\right] \\\\\n",
    "    \\theta &\\leftarrow \\theta - \\eta m \\oslash \\sqrt{s + \\epsilon}\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               784784    \n",
      "=================================================================\n",
      "Total params: 6,604,420\n",
      "Trainable params: 6,589,852\n",
      "Non-trainable params: 14,568\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_8/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_8/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_7/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_7/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_6/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_6/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_5/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_5/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_4/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_4/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_3/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_3/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Rank with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1/start with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1/delta with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/range_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/ListDiff with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/concat/axis with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/concat with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Gather with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Const with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Prod with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Gather_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Const_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "WARNING:tensorflow:Tried to colocate training/Adam/gradients/batch_normalization_2/moments/sufficient_statistics/count_grad/Prod_1 with an op batch_normalization_2/moments/sufficient_statistics/count that had a different device: /device:CPU:0 vs /device:GPU:0. Ignoring colocation property.\n",
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/50\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.0478 - val_loss: 0.0177\n",
      "Epoch 2/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0212 - val_loss: 0.0150\n",
      "Epoch 3/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0192 - val_loss: 0.0133\n",
      "Epoch 4/50\n",
      "42000/42000 [==============================] - 8s 202us/step - loss: 0.0181 - val_loss: 0.0124\n",
      "Epoch 5/50\n",
      "42000/42000 [==============================] - 9s 207us/step - loss: 0.0174 - val_loss: 0.0117\n",
      "Epoch 6/50\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 0.0168 - val_loss: 0.0113\n",
      "Epoch 7/50\n",
      "42000/42000 [==============================] - 8s 201us/step - loss: 0.0165 - val_loss: 0.0110\n",
      "Epoch 8/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0161 - val_loss: 0.0106\n",
      "Epoch 9/50\n",
      "42000/42000 [==============================] - 9s 202us/step - loss: 0.0158 - val_loss: 0.0105\n",
      "Epoch 10/50\n",
      "42000/42000 [==============================] - 8s 199us/step - loss: 0.0156 - val_loss: 0.0104\n",
      "Epoch 11/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0154 - val_loss: 0.0101\n",
      "Epoch 12/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0152 - val_loss: 0.0103\n",
      "Epoch 13/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0151 - val_loss: 0.0099\n",
      "Epoch 14/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0148 - val_loss: 0.0098\n",
      "Epoch 15/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0148 - val_loss: 0.0096\n",
      "Epoch 16/50\n",
      "42000/42000 [==============================] - 9s 204us/step - loss: 0.0146 - val_loss: 0.0096\n",
      "Epoch 17/50\n",
      "42000/42000 [==============================] - 8s 200us/step - loss: 0.0145 - val_loss: 0.0096\n",
      "Epoch 18/50\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 0.0144 - val_loss: 0.0095\n",
      "Epoch 19/50\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 0.0143 - val_loss: 0.0095\n",
      "Epoch 20/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 21/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 22/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 23/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0138 - val_loss: 0.0092\n",
      "Epoch 24/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0137 - val_loss: 0.0091\n",
      "Epoch 25/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0136 - val_loss: 0.0090\n",
      "Epoch 26/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0135 - val_loss: 0.0089\n",
      "Epoch 27/50\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.0135 - val_loss: 0.0090\n",
      "Epoch 28/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0134 - val_loss: 0.0088\n",
      "Epoch 29/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0133 - val_loss: 0.0088\n",
      "Epoch 30/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0132 - val_loss: 0.0088\n",
      "Epoch 31/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0131 - val_loss: 0.0087\n",
      "Epoch 32/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0130 - val_loss: 0.0086\n",
      "Epoch 33/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0130 - val_loss: 0.0087\n",
      "Epoch 34/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0129 - val_loss: 0.0086\n",
      "Epoch 35/50\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.0129 - val_loss: 0.0083\n",
      "Epoch 36/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0128 - val_loss: 0.0085\n",
      "Epoch 37/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0127 - val_loss: 0.0083\n",
      "Epoch 38/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0127 - val_loss: 0.0083\n",
      "Epoch 39/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0126 - val_loss: 0.0083\n",
      "Epoch 40/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0125 - val_loss: 0.0083\n",
      "Epoch 41/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0125 - val_loss: 0.0081\n",
      "Epoch 42/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0124 - val_loss: 0.0082\n",
      "Epoch 43/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0124 - val_loss: 0.0080\n",
      "Epoch 44/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0123 - val_loss: 0.0082\n",
      "Epoch 45/50\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0122 - val_loss: 0.0082\n",
      "Epoch 46/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0122 - val_loss: 0.0080\n",
      "Epoch 47/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0121 - val_loss: 0.0080\n",
      "Epoch 48/50\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.0121 - val_loss: 0.0080\n",
      "Epoch 49/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0121 - val_loss: 0.0079\n",
      "Epoch 50/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.0120 - val_loss: 0.0079\n",
      "The AUROC score for the MNIST classification task with an autoencoder: 99.761064%.\n",
      "The accuracy score for the MNIST classification task with an autoencoder: 96.420000%.\n"
     ]
    }
   ],
   "source": [
    "autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                 n_epoch=50,\n",
    "                                 batch_size=100,\n",
    "                                 encoder_layers=3,\n",
    "                                 decoder_layers=3,\n",
    "                                 n_hidden_units=1000,\n",
    "                                 encoding_dim=500,\n",
    "                                 denoising=None)\n",
    "\n",
    "print(autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_autoencoder = Pipeline(steps=[(\"autoencoder\", autoencoder),\n",
    "                                   (\"scaler_classifier\", scaler_classifier),\n",
    "                                   (\"classifier\", logistic)])\n",
    "\n",
    "pipe_autoencoder = pipe_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                  pipe_autoencoder.predict_proba(X_test), \n",
    "                                  average=\"weighted\")\n",
    "    \n",
    "acc_autoencoder = pipe_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with an autoencoder: %.6f%%.\" % (auroc_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with an autoencoder: %.6f%%.\" % (acc_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Denoising Autoencoders\n",
    "---\n",
    "\n",
    "The idea here is to add some noise to the data and try to learn a set of robust features that can reconstruct the non-noisy data from the noisy data. The MSE objective functions is as follows, $\\frac{1}{N}||D(E(X + \\epsilon)) - X||^{2}_{2}$, where $\\epsilon$ is some noise term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise = 0.10 * np.reshape(np.random.uniform(low=0.0, \n",
    "                                            high=1.0, \n",
    "                                            size=X_train.shape[0] * X_train.shape[1]), \n",
    "                          [X_train.shape[0], X_train.shape[1]])\n",
    "\n",
    "denoising_autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                           n_epoch=50,\n",
    "                                           batch_size=100,\n",
    "                                           encoder_layers=3,\n",
    "                                           decoder_layers=3,\n",
    "                                           n_hidden_units=1000,\n",
    "                                           encoding_dim=500,\n",
    "                                           denoising=noise)\n",
    "\n",
    "print(denoising_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_denoising_autoencoder = Pipeline(steps=[(\"autoencoder\", denoising_autoencoder),\n",
    "                                             (\"scaler_classifier\", scaler_classifier),\n",
    "                                             (\"classifier\", logistic)])\n",
    "\n",
    "pipe_denoising_autoencoder = pipe_denoising_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_denoising_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                            pipe_denoising_autoencoder.predict_proba(X_test), \n",
    "                                            average=\"weighted\")\n",
    "\n",
    "acc_denoising_autoencoder = pipe_denoising_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a denoising autoencoder: %.6f%%.\" % (auroc_denoising_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a denoising autoencoder: %.6f%%.\" % (acc_denoising_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: 1 Dimensional Convolutional Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convolutional_autoencoder = ConvolutionalAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))),\n",
    "                                                     n_epoch=50,\n",
    "                                                     batch_size=100,\n",
    "                                                     encoder_layers=3,\n",
    "                                                     decoder_layers=3,\n",
    "                                                     filters=100,\n",
    "                                                     kernel_size=5,\n",
    "                                                     strides=1,\n",
    "                                                     pool_size=4,\n",
    "                                                     encoding_dim=100,\n",
    "                                                     denoising=None)\n",
    "\n",
    "print(convolutional_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_convolutional_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional_autoencoder),\n",
    "                                                 (\"scaler_classifier\", scaler_classifier),\n",
    "                                                 (\"classifier\", logistic)])\n",
    "\n",
    "pipe_convolutional_autoencoder = pipe_convolutional_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), \n",
    "                                                                    y_train)\n",
    "\n",
    "auroc_convolutional_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                                pipe_convolutional_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))])),\n",
    "                                                average=\"weighted\")\n",
    "\n",
    "acc_convolutional_autoencoder = pipe_convolutional_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a 1 dimensional convolutional autoencoder: %.6f%%.\" % (auroc_convolutional_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a 1 dimensional convolutional autoencoder: %.6f%%.\" % (acc_convolutional_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Sequence to Sequence Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seq2seq_autoencoder = Seq2SeqAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))),\n",
    "                                         n_epoch=50,\n",
    "                                         batch_size=100,\n",
    "                                         encoder_layers=3,\n",
    "                                         decoder_layers=3,\n",
    "                                         n_hidden_units=100,\n",
    "                                         encoding_dim=100,\n",
    "                                         stateful=False,\n",
    "                                         denoising=None)\n",
    "\n",
    "print(seq2seq_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_seq2seq_autoencoder = Pipeline(steps=[(\"autoencoder\", seq2seq_autoencoder),\n",
    "                                           (\"scaler_classifier\", scaler_classifier),\n",
    "                                           (\"classifier\", logistic)])\n",
    "\n",
    "pipe_seq2seq_autoencoder = pipe_seq2seq_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]),\n",
    "                                                        y_train)\n",
    "\n",
    "auroc_seq2seq_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                          pipe_seq2seq_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))])),\n",
    "                                          average=\"weighted\")\n",
    "\n",
    "acc_seq2seq_autoencoder = pipe_seq2seq_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to sequence autoencoder: %.6f%%.\" % (auroc_seq2seq_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to sequence autoencoder: %.6f%%.\" % (acc_seq2seq_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Variational Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoding_dim = 500\n",
    "\n",
    "variational_autoencoder = VariationalAutoencoder(n_feat=X_train.shape[1],\n",
    "                                                 n_epoch=50,\n",
    "                                                 batch_size=100,\n",
    "                                                 encoder_layers=3,\n",
    "                                                 decoder_layers=3,\n",
    "                                                 n_hidden_units=500,\n",
    "                                                 encoding_dim=encoding_dim,\n",
    "                                                 denoising=None)\n",
    "\n",
    "print(variational_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_variational_autoencoder = Pipeline(steps=[(\"autoencoder\", variational_autoencoder),\n",
    "                                               (\"scaler_classifier\", scaler_classifier),\n",
    "                                               (\"classifier\", logistic)])\n",
    "\n",
    "pipe_variational_autoencoder = pipe_variational_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "auroc_variational_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                              pipe_variational_autoencoder.predict_proba(X_test), \n",
    "                                              average=\"weighted\")\n",
    "\n",
    "acc_variational_autoencoder = pipe_variational_autoencoder.score(X_test, y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to variational autoencoder: %.6f%%.\" % (auroc_variational_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to variational autoencoder: %.6f%%.\" % (acc_variational_autoencoder * 100))\n",
    "\n",
    "if encoding_dim == 2:\n",
    "    test_encoded_df = pd.DataFrame(pipe_variational_autoencoder.named_steps[\"autoencoder\"].encoder.predict(X_test))\n",
    "    test_encoded_df[\"Target\"] = y_test\n",
    "    test_encoded_df.columns.values[0:2] = [\"Encoding_1\", \"Encoding_2\"]\n",
    "\n",
    "    scaler_plot = MinMaxScaler(feature_range=(0.25, 0.75))\n",
    "    scaler_plot = scaler_plot.fit(test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]])\n",
    "    test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]] = scaler_plot.transform(test_encoded_df[[\"Encoding_1\", \"Encoding_2\"]])\n",
    "\n",
    "    cluster_plot = ggplot(test_encoded_df) + \\\n",
    "    geom_point(aes(x=\"Encoding_1\", \n",
    "                   y=\"Encoding_2\", \n",
    "                   fill=\"factor(Target)\"),\n",
    "               size=1,\n",
    "               color = \"black\") + \\\n",
    "    xlab(\"Encoding dimension 1\") + \\\n",
    "    ylab(\"Encoding dimension 2\") + \\\n",
    "    ggtitle(\"Variational autoencoder with 2-dimensional encoding\") + \\\n",
    "    theme_matplotlib()\n",
    "    print(cluster_plot)\n",
    "\n",
    "    n = 15\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "    for i, xi in enumerate(grid_x):\n",
    "        for j, yi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = pipe_variational_autoencoder.named_steps[\"autoencoder\"].generator.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.title(\"Variational autoencoder with 2-dimensional encoding\\nGenerating new images\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: 2 Dimensional Convolutional Autoencoders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convolutional2D_autoencoder = Convolutional2DAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1),\n",
    "                                                         n_epoch=5,\n",
    "                                                         batch_size=100,\n",
    "                                                         encoder_layers=3,\n",
    "                                                         decoder_layers=3,\n",
    "                                                         filters=25,\n",
    "                                                         kernel_size=5,\n",
    "                                                         strides=1,\n",
    "                                                         pool_size=4,\n",
    "                                                         denoising=None)\n",
    "\n",
    "print(convolutional2D_autoencoder.autoencoder.summary())\n",
    "\n",
    "pipe_convolutional2D_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional2D_autoencoder),\n",
    "                                                   (\"scaler_classifier\", scaler_classifier),\n",
    "                                                   (\"classifier\", logistic)])\n",
    "\n",
    "pipe_convolutional2D_autoencoder = pipe_convolutional2D_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1]),\n",
    "                                                                        y_train)\n",
    "\n",
    "auroc_convolutional2D_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                                  pipe_convolutional2D_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5)), 1])),\n",
    "                                                  average=\"weighted\")\n",
    "\n",
    "acc_convolutional2D_autoencoder = pipe_convolutional2D_autoencoder.score(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_test.shape[1], 0.5)), int(math.pow(X_test.shape[1], 0.5)), 1]), y_test)\n",
    "\n",
    "print(\"The AUROC score for the MNIST classification task with a sequence to 2 dimensional convolutional autoencoder: %.6f%%.\" % (auroc_convolutional2D_autoencoder * 100))\n",
    "print(\"The accuracy score for the MNIST classification task with a sequence to 2 dimensional convolutional autoencoder: %.6f%%.\" % (acc_convolutional2D_autoencoder * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "\n",
    "1. Goodfellow, I., Bengio, Y. and Courville A. (2016). Deep Learning (MIT Press).\n",
    "2. Geron, A. (2017). Hands-On Machine Learning with Scikit-Learn & Tensorflow (O'Reilly).\n",
    "3. http://scikit-learn.org/stable/#\n",
    "4. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "5. https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano\n",
    "6. https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "7. https://keras.io"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
