{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Hamaad Musharaf Shah.\n",
    "# The following references were used.\n",
    "# https://keras.io\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "# https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano\n",
    "# https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "# http://scikit-learn.org/stable/\n",
    "# Book: Ian Goodfellow, Yoshua Bengio and Aaron Courville, \"Deep Learning\" - http://www.deeplearningbook.org\n",
    "# Book: Aurelien Geron, \"Hands-On Machine Learning with Scikit-Learn & Tensorflow\" - https://www.amazon.co.uk/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras import backend as bkend\n",
    "\n",
    "from autoencoders_keras.vanilla_autoencoder import VanillaAutoencoder\n",
    "from autoencoders_keras.convolutional_autoencoder import ConvolutionalAutoencoder\n",
    "from autoencoders_keras.seq2seq_autoencoder import Seq2SeqAutoencoder\n",
    "from autoencoders_keras.variational_autoencoder import VariationalAutoencoder\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "importlib.reload(bkend)\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X = X.astype(\"float32\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=666)\n",
    "min_X_train = np.apply_along_axis(func1d=np.min, axis=0, arr=X_train)\n",
    "max_X_train = np.apply_along_axis(func1d=np.max, axis=0, arr=X_train) \n",
    "range_X_train = max_X_train - min_X_train + sys.float_info.epsilon\n",
    "X_train = (X_train - min_X_train) / range_X_train\n",
    "X_test = (X_test - min_X_train) / range_X_train\n",
    "\n",
    "scaler_autoencoder = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "scaler_classifier = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "logistic = linear_model.LogisticRegression(random_state=666)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb = lb.fit(y_train.reshape(y_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_base = Pipeline(steps=[(\"scaler_classifier\", scaler_classifier),\n",
    "                            (\"classifier\", logistic)])\n",
    "pipe_base = pipe_base.fit(X_train, y_train)\n",
    "\n",
    "metric_base = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                            pipe_base.predict_proba(X_test), \n",
    "                            average=\"weighted\")\n",
    "print(metric_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                 n_epoch=50,\n",
    "                                 batch_size=100,\n",
    "                                 encoder_layers=3,\n",
    "                                 decoder_layers=3,\n",
    "                                 n_hidden_units=int(X_train.shape[1] / 2),\n",
    "                                 encoding_dim=int(X_train.shape[1] / 2))\n",
    "\n",
    "pipe_autoencoder = Pipeline(steps=[(\"scaler_autoencoder\", scaler_autoencoder),\n",
    "                                   (\"autoencoder\", autoencoder),\n",
    "                                   (\"scaler_classifier\", scaler_classifier),\n",
    "                                   (\"classifier\", logistic)])\n",
    "\n",
    "pipe_autoencoder = pipe_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "metric_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                   pipe_autoencoder.predict_proba(X_test), \n",
    "                                   average=\"weighted\")\n",
    "print(metric_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise = 0.01 * np.reshape(np.random.uniform(low=0.0, \n",
    "                                            high=1.0, \n",
    "                                            size=X_train.shape[0] * X_train.shape[1]), \n",
    "                          [X_train.shape[0], X_train.shape[1]])\n",
    "\n",
    "denoising_autoencoder = VanillaAutoencoder(n_feat=X_train.shape[1],\n",
    "                                           n_epoch=50,\n",
    "                                           batch_size=100,\n",
    "                                           encoder_layers=3,\n",
    "                                           decoder_layers=3,\n",
    "                                           n_hidden_units=int(X_train.shape[1] / 2),\n",
    "                                           encoding_dim=int(X_train.shape[1] / 2))\n",
    "\n",
    "pipe_denoising_autoencoder = Pipeline(steps=[(\"scaler_autoencoder\", scaler_autoencoder),\n",
    "                                             (\"autoencoder\", denoising_autoencoder),\n",
    "                                             (\"scaler_classifier\", scaler_classifier),\n",
    "                                             (\"classifier\", logistic)])\n",
    "\n",
    "pipe_denoising_autoencoder = pipe_denoising_autoencoder.fit(X_train + noise, y_train)\n",
    "\n",
    "metric_denoising_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                             pipe_denoising_autoencoder.predict_proba(X_test), \n",
    "                                             average=\"weighted\")\n",
    "print(metric_denoising_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_autoencoder = ConvolutionalAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))),\n",
    "                                                     n_epoch=50,\n",
    "                                                     batch_size=100,\n",
    "                                                     encoder_layers=3,\n",
    "                                                     decoder_layers=3,\n",
    "                                                     filters=100,\n",
    "                                                     kernel_size=5,\n",
    "                                                     strides=1,\n",
    "                                                     pool_size=4,\n",
    "                                                     encoding_dim=100)\n",
    "\n",
    "pipe_convolutional_autoencoder = Pipeline(steps=[(\"autoencoder\", convolutional_autoencoder),\n",
    "                                                 (\"scaler_classifier\", scaler_classifier),\n",
    "                                                 (\"classifier\", logistic)])\n",
    "\n",
    "pipe_convolutional_autoencoder = pipe_convolutional_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]), \n",
    "                                                                    y_train)\n",
    "\n",
    "metric_convolutional_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                                 pipe_convolutional_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))])), \n",
    "                                                 average=\"weighted\")\n",
    "print(metric_convolutional_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq_autoencoder = Seq2SeqAutoencoder(input_shape=(int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))),\n",
    "                                         n_epoch=2,\n",
    "                                         batch_size=100,\n",
    "                                         encoder_layers=3,\n",
    "                                         decoder_layers=3,\n",
    "                                         n_hidden_units=100,\n",
    "                                         encoding_dim=100)\n",
    "\n",
    "pipe_seq2seq_autoencoder = Pipeline(steps=[(\"autoencoder\", seq2seq_autoencoder),\n",
    "                                           (\"scaler_classifier\", scaler_classifier),\n",
    "                                           (\"classifier\", logistic)])\n",
    "\n",
    "pipe_seq2seq_autoencoder = pipe_seq2seq_autoencoder.fit(np.reshape(X_train, [X_train.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))]),\n",
    "                                                        y_train)\n",
    "\n",
    "metric_seq2seq_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                           pipe_seq2seq_autoencoder.predict_proba(np.reshape(X_test, [X_test.shape[0], int(math.pow(X_train.shape[1], 0.5)), int(math.pow(X_train.shape[1], 0.5))])), \n",
    "                                           average=\"weighted\")\n",
    "print(metric_seq2seq_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variational_autoencoder = VariationalAutoencoder(n_feat=X_train.shape[1],\n",
    "                                                 n_epoch=50,\n",
    "                                                 batch_size=100,\n",
    "                                                 encoder_layers=3,\n",
    "                                                 decoder_layers=3,\n",
    "                                                 n_hidden_units=int(X_train.shape[1] / 2),\n",
    "                                                 encoding_dim=int(X_train.shape[1] / 2))\n",
    "\n",
    "pipe_variational_autoencoder = Pipeline(steps=[(\"scaler_autoencoder\", scaler_autoencoder),\n",
    "                                               (\"autoencoder\", variational_autoencoder),\n",
    "                                               (\"scaler_classifier\", scaler_classifier),\n",
    "                                               (\"classifier\", logistic)])\n",
    "\n",
    "pipe_variational_autoencoder = pipe_variational_autoencoder.fit(X_train, y_train)\n",
    "\n",
    "metric_variational_autoencoder = roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "                                               pipe_variational_autoencoder.predict_proba(X_test), \n",
    "                                               average=\"weighted\")\n",
    "print(metric_variational_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In development.\n",
    "# Some transfer learning stuff.\n",
    "# Probably will come in as another repo.\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "cifar = cifar10.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = cifar\n",
    "cifar[0][0][0, :, :, 0]\n",
    "x_train.shape[1:]\n",
    "\n",
    "autoencoder.layers[0:10]\n",
    "\n",
    "for layer in autoencoder.layers[0:10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "autoencoder.layers[9].output\n",
    "\n",
    "classifier_output = autoencoder.layers[9].output\n",
    "classifier_output = Dense(100, activation=\"elu\")(classifier_output)\n",
    "classifier_output = Dense(100, activation=\"elu\")(classifier_output)\n",
    "classifier_output = Dense(10, activation=\"softmax\")(classifier_output)\n",
    "\n",
    "model_final = Model(autoencoder.input, classifier_output)\n",
    "model_final.compile(loss = \"categorical_crossentropy\", \n",
    "                    optimizer=keras.optimizers.Adam(), \n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "model_final.fit(X_train, keras.utils.to_categorical(y_train, 10),\n",
    "                validation_split=0.3,\n",
    "                epochs=50,\n",
    "                batch_size=100,\n",
    "                shuffle=True,\n",
    "                callbacks=callbacks_list, \n",
    "                verbose=2)\n",
    "\n",
    "model_final.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(lb.transform(y_test.reshape(y_test.shape[0], 1)), \n",
    "              model_final.predict(X_test), \n",
    "              average=\"weighted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
